{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to /home/filipp/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import pathlib\n",
    "import nltk\n",
    "import numpy as np\n",
    "from unicodedata import normalize\n",
    "import collections\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from simplifier import *\n",
    "\n",
    "nltk.download('words')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_data = 'data-simplification/wikilarge/wiki.full.aner.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_src = open(path_data+'train.src', \"r\").readlines()\n",
    "train_dst = open(path_data+'train.dst', \"r\").readlines()\n",
    "train_ori_src = open(path_data+'ori.train.src', \"r\").readlines()\n",
    "train_ori_dst = open(path_data+'ori.train.dst', \"r\").readlines()\n",
    "\n",
    "valid_src = open(path_data+'valid.src', \"r\").readlines()\n",
    "valid_dst = open(path_data+'valid.dst', \"r\").readlines()\n",
    "valid_ori_src = open(path_data+'ori.valid.src', \"r\").readlines()\n",
    "valid_ori_dst = open(path_data+'ori.valid.dst', \"r\").readlines()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison of datasets; will work with original first. Named entity identification is applied to get the modified dataset. Might facilitate trainining working with this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ORGANIZATION@1 crews conducted backburning operations to ensure containment of the fire on NUMBER@1 February , warning residents of areas between LOCATION@1 and Warragul about smoke from those fires .\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_src[114]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'DSE crews conducted backburning operations to ensure containment of the fire on 9 February , warning residents of areas between Pakenham and Warragul about smoke from those fires .\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ori_src[114]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'People between Pakenham and Warragul were warned about smoke from those fires .\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ori_dst[114]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'People between LOCATION@1 and Warragul were warned about smoke from those fires .\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dst[114]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Thousands of young people from all over the world went to San Francisco to help create a hippie counterculture . The Summer of Love made the rest of America much more aware of the hippie movement .\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_ori_dst[11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_index = {}\n",
    "file = os.path.join(str(pathlib.Path.home())+\"/GloVe\",\"glove.6B.100d.txt\")\n",
    "f= open(file)\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype=\"float32\")\n",
    "    embedding_index[word] = coefs\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A glimpse at the representations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the [-0.038194 -0.24487   0.72812  -0.39961   0.083172  0.043953 -0.39141\n",
      "  0.3344   -0.57545   0.087459  0.28787  -0.06731   0.30906  -0.26384\n",
      " -0.13231  -0.20757   0.33395  -0.33848  -0.31743  -0.48336   0.1464\n",
      " -0.37304   0.34577   0.052041  0.44946  -0.46971   0.02628  -0.54155\n",
      " -0.15518  -0.14107  -0.039722  0.28277   0.14393   0.23464  -0.31021\n",
      "  0.086173  0.20397   0.52624   0.17164  -0.082378 -0.71787  -0.41531\n",
      "  0.20335  -0.12763   0.41367   0.55187   0.57908  -0.33477  -0.36559\n",
      " -0.54857  -0.062892  0.26584   0.30205   0.99775  -0.80481  -3.0243\n",
      "  0.01254  -0.36942   2.2167    0.72201  -0.24978   0.92136   0.034514\n",
      "  0.46745   1.1079   -0.19358  -0.074575  0.23353  -0.052062 -0.22044\n",
      "  0.057162 -0.15806  -0.30798  -0.41625   0.37972   0.15006  -0.53212\n",
      " -0.2055   -1.2526    0.071624  0.70565   0.49744  -0.42063   0.26148\n",
      " -1.538    -0.30223  -0.073438 -0.28312   0.37104  -0.25217   0.016215\n",
      " -0.017099 -0.38984   0.87424  -0.72569  -0.51058  -0.52028  -0.1459\n",
      "  0.8278    0.27062 ]\n",
      "(100,)\n",
      ", [-0.10767    0.11053    0.59812   -0.54361    0.67396    0.10663\n",
      "  0.038867   0.35481    0.06351   -0.094189   0.15786   -0.81665\n",
      "  0.14172    0.21939    0.58505   -0.52158    0.22783   -0.16642\n",
      " -0.68228    0.3587     0.42568    0.19021    0.91963    0.57555\n",
      "  0.46185    0.42363   -0.095399  -0.42749   -0.16567   -0.056842\n",
      " -0.29595    0.26037   -0.26606   -0.070404  -0.27662    0.15821\n",
      "  0.69825    0.43081    0.27952   -0.45437   -0.33801   -0.58184\n",
      "  0.22364   -0.5778    -0.26862   -0.20425    0.56394   -0.58524\n",
      " -0.14365   -0.64218    0.0054697 -0.35248    0.16162    1.1796\n",
      " -0.47674   -2.7553    -0.1321    -0.047729   1.0655     1.1034\n",
      " -0.2208     0.18669    0.13177    0.15117    0.7131    -0.35215\n",
      "  0.91348    0.61783    0.70992    0.23955   -0.14571   -0.37859\n",
      " -0.045959  -0.47368    0.2385     0.20536   -0.18996    0.32507\n",
      " -1.1112    -0.36341    0.98679   -0.084776  -0.54008    0.11726\n",
      " -1.0194    -0.24424    0.12771    0.013884   0.080374  -0.35414\n",
      "  0.34951   -0.7226     0.37549    0.4441    -0.99059    0.61214\n",
      " -0.35111   -0.83155    0.45293    0.082577 ]\n",
      "(100,)\n",
      ". [-0.33979    0.20941    0.46348   -0.64792   -0.38377    0.038034\n",
      "  0.17127    0.15978    0.46619   -0.019169   0.41479   -0.34349\n",
      "  0.26872    0.04464    0.42131   -0.41032    0.15459    0.022239\n",
      " -0.64653    0.25256    0.043136  -0.19445    0.46516    0.45651\n",
      "  0.68588    0.091295   0.21875   -0.70351    0.16785   -0.35079\n",
      " -0.12634    0.66384   -0.2582     0.036542  -0.13605    0.40253\n",
      "  0.14289    0.38132   -0.12283   -0.45886   -0.25282   -0.30432\n",
      " -0.11215   -0.26182   -0.22482   -0.44554    0.2991    -0.85612\n",
      " -0.14503   -0.49086    0.0082973 -0.17491    0.27524    1.4401\n",
      " -0.21239   -2.8435    -0.27958   -0.45722    1.6386     0.78808\n",
      " -0.55262    0.65       0.086426   0.39012    1.0632    -0.35379\n",
      "  0.48328    0.346      0.84174    0.098707  -0.24213   -0.27053\n",
      "  0.045287  -0.40147    0.11395    0.0062226  0.036673   0.018518\n",
      " -1.0213    -0.20806    0.64072   -0.068763  -0.58635    0.33476\n",
      " -1.1432    -0.1148    -0.25091   -0.45907   -0.096819  -0.17946\n",
      " -0.063351  -0.67412   -0.068895   0.53604   -0.87773    0.31802\n",
      " -0.39242   -0.23394    0.47298   -0.028803 ]\n",
      "(100,)\n",
      "of [-0.1529   -0.24279   0.89837   0.16996   0.53516   0.48784  -0.58826\n",
      " -0.17982  -1.3581    0.42541   0.15377   0.24215   0.13474   0.41193\n",
      "  0.67043  -0.56418   0.42985  -0.012183 -0.11677   0.31781   0.054177\n",
      " -0.054273  0.35516  -0.30241   0.31434  -0.33846   0.71715  -0.26855\n",
      " -0.15837  -0.47467   0.051581 -0.33252   0.15003  -0.1299   -0.54617\n",
      " -0.37843   0.64261   0.82187  -0.080006  0.078479 -0.96976  -0.57741\n",
      "  0.56491  -0.39873  -0.057099  0.19743   0.065706 -0.48092  -0.20125\n",
      " -0.40834   0.39456  -0.02642  -0.11838   1.012    -0.53171  -2.7474\n",
      " -0.042981 -0.74849   1.7574    0.59085   0.04885   0.78267   0.38497\n",
      "  0.42097   0.67882   0.10337   0.6328   -0.026595  0.58647  -0.44332\n",
      "  0.33057  -0.12022  -0.55645   0.073611  0.20915   0.43395  -0.012761\n",
      "  0.089874 -1.7991    0.084808  0.77112   0.63105  -0.90685   0.60326\n",
      " -1.7515    0.18596  -0.50687  -0.70203   0.66578  -0.81304   0.18712\n",
      " -0.018488 -0.26757   0.727    -0.59363  -0.34839  -0.56094  -0.591\n",
      "  1.0039    0.20664 ]\n",
      "(100,)\n",
      "to [-1.8970e-01  5.0024e-02  1.9084e-01 -4.9184e-02 -8.9737e-02  2.1006e-01\n",
      " -5.4952e-01  9.8377e-02 -2.0135e-01  3.4241e-01 -9.2677e-02  1.6100e-01\n",
      " -1.3268e-01 -2.8160e-01  1.8737e-01 -4.2959e-01  9.6039e-01  1.3972e-01\n",
      " -1.0781e+00  4.0518e-01  5.0539e-01 -5.5064e-01  4.8440e-01  3.8044e-01\n",
      " -2.9055e-03 -3.4942e-01 -9.9696e-02 -7.8368e-01  1.0363e+00 -2.3140e-01\n",
      " -4.7121e-01  5.7126e-01 -2.1454e-01  3.5958e-01 -4.8319e-01  1.0875e+00\n",
      "  2.8524e-01  1.2447e-01 -3.9248e-02 -7.6732e-02 -7.6343e-01 -3.2409e-01\n",
      " -5.7490e-01 -1.0893e+00 -4.1811e-01  4.5120e-01  1.2112e-01 -5.1367e-01\n",
      " -1.3349e-01 -1.1378e+00 -2.8768e-01  1.6774e-01  5.5804e-01  1.5387e+00\n",
      "  1.8859e-02 -2.9721e+00 -2.4216e-01 -9.2495e-01  2.1992e+00  2.8234e-01\n",
      " -3.4780e-01  5.1621e-01 -4.3387e-01  3.6852e-01  7.4573e-01  7.2102e-02\n",
      "  2.7931e-01  9.2569e-01 -5.0336e-02 -8.5856e-01 -1.3580e-01 -9.2551e-01\n",
      " -3.3991e-01 -1.0394e+00 -6.7203e-02 -2.1379e-01 -4.7690e-01  2.1377e-01\n",
      " -8.4008e-01  5.2536e-02  5.9298e-01  2.9604e-01 -6.7644e-01  1.3916e-01\n",
      " -1.5504e+00 -2.0765e-01  7.2220e-01  5.2056e-01 -7.6221e-02 -1.5194e-01\n",
      " -1.3134e-01  5.8617e-02 -3.1869e-01 -6.1419e-01 -6.2393e-01 -4.1548e-01\n",
      " -3.8175e-02 -3.9804e-01  4.7647e-01 -1.5983e-01]\n",
      "(100,)\n",
      "and [-0.071953  0.23127   0.023731 -0.50638   0.33923   0.1959   -0.32943\n",
      "  0.18364  -0.18057   0.28963   0.20448  -0.5496    0.27399   0.58327\n",
      "  0.20468  -0.49228   0.19974  -0.070237 -0.88049   0.29485   0.14071\n",
      " -0.1009    0.99449   0.36973   0.44554   0.28998  -0.1376   -0.56365\n",
      " -0.029365 -0.4122   -0.25269   0.63181  -0.44767   0.24363  -0.10813\n",
      "  0.25164   0.46967   0.3755   -0.23613  -0.14129  -0.44537  -0.65737\n",
      " -0.042421 -0.28636  -0.28811   0.063766  0.20281  -0.53542   0.41307\n",
      " -0.59722  -0.38614   0.19389  -0.17809   1.6618   -0.011819 -2.3737\n",
      "  0.058427 -0.2698    1.2823    0.81925  -0.22322   0.72932  -0.053211\n",
      "  0.43507   0.85011  -0.42935   0.92664   0.39051   1.0585   -0.24561\n",
      " -0.18265  -0.5328    0.059518 -0.66019   0.18991   0.28836  -0.2434\n",
      "  0.52784  -0.65762  -0.14081   1.0491    0.5134   -0.23816   0.69895\n",
      " -1.4813   -0.2487   -0.17936  -0.059137 -0.08056  -0.48782   0.014487\n",
      " -0.6259   -0.32367   0.41862  -1.0807    0.46742  -0.49931  -0.71895\n",
      "  0.86894   0.19539 ]\n",
      "(100,)\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "for key, value in embedding_index.items():\n",
    "    if i>5:\n",
    "        break\n",
    "    print(key, value)\n",
    "    print(value.shape)\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_txt = clean(train_ori_src)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"There is manuscript evidence that Austen continued to work on these pieces as late as the period 1809 â '' 11 , and that her niece and nephew , Anna and James Edward Austen , made further additions as late as 1814 .\\n\",\n",
       " \"In a remarkable comparative analysis , Mandaean scholar Säve-Söderberg demonstrated that Mani 's Psalms of Thomas were closely related to Mandaean texts .\\n\"]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ori_src[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'there is manuscript evidence that austen continued to work on these pieces as late as the period a and that her niece and nephew anna and james edward austen made further additions as late asin a remarkable comparative analysis mandaean scholar demonstrated that mani psalms of thomas were closely related to mandaean textsbefore persephone was released to hermes who had been sent to retrieve her ha'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_txt[0:400]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['there', 'is', 'manuscript', 'evidence', 'that', 'austen', 'continued', 'to', 'work', 'on', 'these', 'pieces', 'as', 'late', 'as', 'the', 'period', 'a', 'and', 'that'] \n",
      "\n",
      "Total tokens :  5742368\n"
     ]
    }
   ],
   "source": [
    "token_list_src = nltk.word_tokenize(clean_txt)\n",
    "print(token_list_src[0:20],\"\\n\")\n",
    "print(\"Total tokens : \", len(token_list_src))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['there', 'is', 'manuscript', 'evidence', 'that', 'continued', 'to', 'work', 'on', 'these', 'pieces', 'as', 'late', 'as', 'the', 'period', 'a', 'and', 'that', 'her'] \n",
      "\n",
      "Total tokens :  4961048\n"
     ]
    }
   ],
   "source": [
    "# comparison with source tokens when entities replaced by placeholders\n",
    "token_list_with_ent_src = nltk.word_tokenize(clean(train_src))\n",
    "print(token_list_with_ent_src[0:20],\"\\n\")\n",
    "print(\"Total tokens : \", len(token_list_with_ent_src))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "332341"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#total vocabulary size in source\n",
    "len(set(token_list_src))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Looking at the frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27266\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('the', 414529),\n",
       " ('of', 238408),\n",
       " ('and', 180703),\n",
       " ('in', 180597),\n",
       " ('a', 158684),\n",
       " ('is', 116580),\n",
       " ('to', 102777),\n",
       " ('was', 61810),\n",
       " ('as', 55788),\n",
       " ('by', 45144)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "token_list_src_over_threshold = build_vocabulary(tokenize(clean(train_ori_src)), threshold=10)\n",
    "vocabulary_size_source = len(token_list_src_over_threshold) + 1\n",
    "print(vocabulary_size_source)\n",
    "token_list_src_over_threshold[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Which words/tokens from corpus are missing in GloVe; obviously lots of misspelled/not properly separated words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1871"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "not_in_GloVe = []\n",
    "for word, _ in token_list_src_over_threshold:\n",
    "    if word not in embedding_index:\n",
    "        not_in_GloVe.append(word)\n",
    "len(not_in_GloVe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using spell checker a la Norvig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spellchecker import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['umbrella', 'as']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "separate_words(\"umbrellaas\", embedding_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Separating a word not in the embedding might give two words in the embedding. Run a couple of times, to see that the number of words not in GloVe decreases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "954"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for word in not_in_GloVe:\n",
    "    if len(separate_words(word, embedding_index))>1:\n",
    "        not_in_GloVe.remove(word)\n",
    "        \n",
    "len(not_in_GloVe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['francein',\n",
       " 'franceit',\n",
       " 'francehe',\n",
       " 'germanythe',\n",
       " 'switzerlandthe',\n",
       " 'timethe',\n",
       " 'englandthe',\n",
       " 'citythe',\n",
       " 'thethe',\n",
       " 'franceis',\n",
       " 'francethis',\n",
       " 'centurythe',\n",
       " 'leaguethe',\n",
       " 'areathe',\n",
       " 'calendarthe',\n",
       " 'francethey',\n",
       " 'maythe',\n",
       " 'francendash',\n",
       " 'octoberthe',\n",
       " 'julythe']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "not_in_GloVe[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "improved_token_list = improve_token_list([x[0] for x in token_list_src_over_threshold], \\\n",
    "                                         embedding_index, separate_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26253"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "improved_token_list = list((set(improved_token_list)))\n",
    "len(improved_token_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating token-index-dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_src = build_word2index(improved_token_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26254"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer_src)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### embedding matrix to use the GloVe embedding in the model later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_size = 100 # GloVe 100d\n",
    "vocabulary_size_source = len(tokenizer_src)+1\n",
    "embedding_matrix = np.zeros((vocabulary_size_source, embedding_size))\n",
    "for word, idx in tokenizer_src.items():\n",
    "    embedding_vector = embedding_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[idx] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Sirius left Portsmouth on 13 May 1787 , and arrived at Port Jackson on 26 January 1788 .\\n'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = train_ori_src[120]\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer(test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's look at sentence lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "lengths=[]\n",
    "lengths = [len((tokenizer(x))) for x in train_ori_src]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min 0\n",
      "mean 22.678625650299256\n",
      "max 104\n",
      "std 11.685398987054729\n"
     ]
    }
   ],
   "source": [
    "print(\"min\", np.min(lengths))\n",
    "print(\"mean\", np.mean(lengths))\n",
    "print(\"max\", np.max(lengths))\n",
    "print(\"std\", np.std(lengths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYMAAAD4CAYAAAAO9oqkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+j8jraAAAV2klEQVR4nO3df6zd9X3f8edrdiEhbWIDHqO2md3hpjJoWegtuMpWpdCBgSjmDxqBuuFlVi2tJE27SIlppaElQYItKg1awuSBi4kifozSYgUS5hE2NKkYLiHlN+UGSHwtiG9iQ7pFgzh974/zMTu5uRf7nnN8f5z7fEhX93zf38/3nM9XX8uv+/l8P+ecVBWSpMXt7811ByRJc88wkCQZBpIkw0CShGEgSQKWznUHenXyySfXmjVr5robkrSgPPbYY9+vqhWT6ws2DNasWcPo6Ohcd0OSFpQk35mq7jSRJMkwkCQZBpIkDANJEoaBJImjCIMkO5LsT/LUpPrHkzyX5Okk/6GrflWSsSTPJ7mgq76x1caSbOuqr02yp9XvSHLcoE5OknR0jmZkcAuwsbuQ5DeBTcD7quoM4POtvh64DDijHfOlJEuSLAG+CFwIrAcub20BrgOur6rTgYPAln5PSpI0M0cMg6p6CDgwqfxvgGur6o3WZn+rbwJur6o3quolYAw4u/2MVdWLVfUmcDuwKUmAc4G72vE7gUv6PCdJ0gz1es/gl4F/1qZ3/meSX2v1lcDernbjrTZd/STgtao6NKk+pSRbk4wmGZ2YmOix65KkyXp9B/JS4ERgA/BrwJ1JfmlgvZpGVW0HtgOMjIz4rTyzZM22e996/PK1F89hTyQdK72GwThwd3W+Ju2RJH8HnAzsA1Z3tVvVakxT/wGwLMnSNjrobi9JmiW9ThP9JfCbAEl+GTgO+D6wC7gsyfFJ1gLrgEeAR4F1beXQcXRuMu9qYfIgcGl73s3APb2ejCSpN0ccGSS5DfggcHKSceBqYAewoy03fRPY3P5jfzrJncAzwCHgyqr6SXuejwH3A0uAHVX1dHuJTwO3J/kc8Dhw8wDPT5J0FI4YBlV1+TS7/sU07a8Brpmifh9w3xT1F+msNpIkzRHfgSxJMgwkSYaBJAnDQJKEYSBJwjCQJGEYSJIwDCRJGAaSJAwDSRKGgSQJw0CShGEgScIwkCRhGEiSMAwkSRgGkiSOIgyS7Eiyv33F5eR9n0xSSU5u20lyQ5KxJE8kOaur7eYkL7SfzV31X03yZDvmhiQZ1MlJko7O0YwMbgE2Ti4mWQ2cD3y3q3whsK79bAVubG1PpPPdyefQ+YrLq5Msb8fcCPxu13E/81qSpGPriGFQVQ8BB6bYdT3wKaC6apuAW6vjYWBZklOBC4DdVXWgqg4Cu4GNbd+7q+rhqirgVuCS/k5JkjRTPd0zSLIJ2FdVfz1p10pgb9f2eKu9XX18ivp0r7s1yWiS0YmJiV66LkmawozDIMkJwB8B/27w3Xl7VbW9qkaqamTFihWz/fKSNLR6GRn8I2At8NdJXgZWAd9M8g+AfcDqrrarWu3t6qumqEuSZtGMw6Cqnqyqv19Va6pqDZ2pnbOq6lVgF3BFW1W0AXi9ql4B7gfOT7K83Tg+H7i/7fthkg1tFdEVwD0DOjdJ0lE6mqWltwF/Bbw3yXiSLW/T/D7gRWAM+C/A7wFU1QHgs8Cj7eczrUZrc1M75tvA13o7FUlSr5YeqUFVXX6E/Wu6Hhdw5TTtdgA7pqiPAmceqR+SpGPHdyBLkgwDSZJhIEnCMJAkYRhIkjAMJEkYBpIkDANJEoaBJAnDQJKEYSBJwjCQJGEYSJIwDCRJGAaSJAwDSRKGgSSJo/vayx1J9id5qqv2H5M8l+SJJH+RZFnXvquSjCV5PskFXfWNrTaWZFtXfW2SPa1+R5LjBnmCw2TNtnvf+pGkQTqakcEtwMZJtd3AmVX1j4G/Aa4CSLIeuAw4ox3zpSRLkiwBvghcCKwHLm9tAa4Drq+q04GDwNt9x7KOwMCQ1IsjhkFVPQQcmFT7b1V1qG0+DKxqjzcBt1fVG1X1Ep0vuT+7/YxV1YtV9SZwO7ApSYBzgbva8TuBS/o8J0nSDA3insG/Br7WHq8E9nbtG2+16eonAa91Bcvh+pSSbE0ymmR0YmJiAF2XJEGfYZDkj4FDwFcG0523V1Xbq2qkqkZWrFgxGy8pSYvC0l4PTPKvgA8B51VVtfI+YHVXs1WtxjT1HwDLkixto4Pu9pKkWdLTyCDJRuBTwIer6kddu3YBlyU5PslaYB3wCPAosK6tHDqOzk3mXS1EHgQubcdvBu7p7VQkSb06mqWltwF/Bbw3yXiSLcB/An4B2J3kW0n+M0BVPQ3cCTwDfB24sqp+0v7q/xhwP/AscGdrC/Bp4N8mGaNzD+HmgZ6hJOmIjjhNVFWXT1Ge9j/sqroGuGaK+n3AfVPUX6Sz2kgz0L109OVrL57DnkgaBj3fM9D84XsKJPXLj6OQJDkymO/8q1/SbHBkIEkyDCRJThPNO04LSZoLhsE8YABImmtOE0mSHBksFr5JTdLbMQyGmNNPko6W00SSJMNAkmQYSJIwDCRJGAaSJAwDSRKGgSSJo/vayx1J9id5qqt2YpLdSV5ov5e3epLckGQsyRNJzuo6ZnNr/0KSzV31X03yZDvmhiQZ9Enqp63Zdu9bP5IERzcyuAXYOKm2DXigqtYBD7RtgAuBde1nK3AjdMIDuBo4h85XXF59OEBam9/tOm7ya0mSjrEjhkFVPQQcmFTeBOxsj3cCl3TVb62Oh4FlSU4FLgB2V9WBqjoI7AY2tn3vrqqHq6qAW7ueS5I0S3q9Z3BKVb3SHr8KnNIerwT2drUbb7W3q49PUZ9Skq1JRpOMTkxM9Nh1SdJkfd9Abn/R1wD6cjSvtb2qRqpqZMWKFbPxkpK0KPQaBt9rUzy03/tbfR+wuqvdqlZ7u/qqKeqSpFnUaxjsAg6vCNoM3NNVv6KtKtoAvN6mk+4Hzk+yvN04Ph+4v+37YZINbRXRFV3PJUmaJUf8COsktwEfBE5OMk5nVdC1wJ1JtgDfAT7Smt8HXASMAT8CPgpQVQeSfBZ4tLX7TFUdvin9e3RWLL0T+Fr7GXou65Q0nxwxDKrq8ml2nTdF2wKunOZ5dgA7pqiPAmceqR+SpGPHL7dZ5PwGNEngx1FIknBkoC6OEqTFy5GBJMkwkCQZBpIkDANJEt5A1jR8U5y0uDgykCQ5MphN/rUtab5yZCBJMgwkSYaBJAnDQJKEYSBJwtVEx5wriCQtBI4MJEn9hUGSP0zydJKnktyW5B1J1ibZk2QsyR1Jjmttj2/bY23/mq7nuarVn09yQX+nJEmaqZ7DIMlK4PeBkao6E1gCXAZcB1xfVacDB4Et7ZAtwMFWv761I8n6dtwZwEbgS0mW9NovSdLM9TtNtBR4Z5KlwAnAK8C5wF1t/07gkvZ4U9um7T8vSVr99qp6o6peAsaAs/vslyRpBnoOg6raB3we+C6dEHgdeAx4raoOtWbjwMr2eCWwtx17qLU/qbs+xTE/JcnWJKNJRicmJnrtuiRpkn6miZbT+at+LfCLwLvoTPMcM1W1vapGqmpkxYoVx/KlJGlR6Wea6LeAl6pqoqp+DNwNfABY1qaNAFYB+9rjfcBqgLb/PcAPuutTHCNJmgX9hMF3gQ1JTmhz/+cBzwAPApe2NpuBe9rjXW2btv8bVVWtfllbbbQWWAc80ke/JEkz1PObzqpqT5K7gG8Ch4DHge3AvcDtST7Xaje3Q24GvpxkDDhAZwURVfV0kjvpBMkh4Mqq+kmv/ZIkzVxf70CuqquBqyeVX2SK1UBV9X+B357mea4BrumnL5Kk3vkOZEmSYSBJMgwkSRgGkiT8CGvNUPdHcr987cVz2BNJg+TIQJJkGEiSDANJEt4zUB+8fyAND0cGkiTDQJJkGEiSMAwkSRgGkiQMA0kSLi3VgLjMVFrYHBlIkvoLgyTLktyV5Lkkzyb59SQnJtmd5IX2e3lrmyQ3JBlL8kSSs7qeZ3Nr/0KSzdO/oiTpWOh3ZPAF4OtV9SvA+4BngW3AA1W1DnigbQNcSOfL7tcBW4EbAZKcSOerM8+h83WZVx8OEEnS7Og5DJK8B/gN2hfeV9WbVfUasAnY2ZrtBC5pjzcBt1bHw8CyJKcCFwC7q+pAVR0EdgMbe+2XJGnm+hkZrAUmgD9L8niSm5K8Czilql5pbV4FTmmPVwJ7u44fb7Xp6j8jydYko0lGJyYm+ui6JKlbP6uJlgJnAR+vqj1JvsD/nxICoKoqSfXTwUnPtx3YDjAyMjKw5x207pU1krQQ9BMG48B4Ve1p23fRCYPvJTm1ql5p00D72/59wOqu41e12j7gg5Pq/6OPfmmOucxUWnh6niaqqleBvUne20rnAc8Au4DDK4I2A/e0x7uAK9qqog3A62066X7g/CTL243j81tNkjRL+n3T2ceBryQ5DngR+CidgLkzyRbgO8BHWtv7gIuAMeBHrS1VdSDJZ4FHW7vPVNWBPvslSZqBvsKgqr4FjEyx67wp2hZw5TTPswPY0U9fJEm98x3IkiTDQJLkB9XpGHNlkbQwODKQJBkGkiTDQJKEYSBJwjCQJOFqooHxw+mOzJVF0vzlyECSZBhIkgwDSRKGgSQJbyBrjngzWZpfHBlIkgwDSZJhIEliAGGQZEmSx5N8tW2vTbInyViSO9pXYpLk+LY91vav6XqOq1r9+SQX9NsnLSxrtt371o+kuTGIkcEngGe7tq8Drq+q04GDwJZW3wIcbPXrWzuSrAcuA84ANgJfSrJkAP2SJB2lvsIgySrgYuCmth3gXOCu1mQncEl7vKlt0/af19pvAm6vqjeq6iVgDDi7n35Jkmam36Wlfwp8CviFtn0S8FpVHWrb48DK9nglsBegqg4leb21Xwk83PWc3cf8lCRbga0Ap512Wp9d13zkklNpbvQ8MkjyIWB/VT02wP68raraXlUjVTWyYsWK2XpZSRp6/YwMPgB8OMlFwDuAdwNfAJYlWdpGB6uAfa39PmA1MJ5kKfAe4Add9cO6j5nXvOEpaVj0PDKoqquqalVVraFzA/gbVfU7wIPApa3ZZuCe9nhX26bt/0ZVVatf1lYbrQXWAY/02i9J0swdi4+j+DRwe5LPAY8DN7f6zcCXk4wBB+gECFX1dJI7gWeAQ8CVVfWTY9AvSdI00vnjfOEZGRmp0dHROe2D00Szx5vJ0mAkeayqRibXfQeyJMkwkCQZBpIkDANJEoaBJAm/6WzGXEEkaRgZBloQJoewS02lwXKaSJLkyEALk59uKg2WYaAFz2CQ+uc0kSTJMJAkGQaSJAwDSRLeQNaQ8Way1BtHBpIkRwZHw4+gkDTseh4ZJFmd5MEkzyR5OsknWv3EJLuTvNB+L2/1JLkhyViSJ5Kc1fVcm1v7F5Jsnu41JUnHRj/TRIeAT1bVemADcGWS9cA24IGqWgc80LYBLqTzZffrgK3AjdAJD+Bq4BzgbODqwwEiSZodPYdBVb1SVd9sj/8WeBZYCWwCdrZmO4FL2uNNwK3V8TCwLMmpwAXA7qo6UFUHgd3Axl77JUmauYHcM0iyBng/sAc4papeabteBU5pj1cCe7sOG2+16epTvc5WOqMKTjvttEF0fVreJ1j4pruGrjKSflbfYZDk54E/B/6gqn6Y5K19VVVJqt/X6Hq+7cB2gJGRkYE8r0sRJanPpaVJfo5OEHylqu5u5e+16R/a7/2tvg9Y3XX4qlabri5JmiX9rCYKcDPwbFX9SdeuXcDhFUGbgXu66le0VUUbgNfbdNL9wPlJlrcbx+e3mnRMrNl271s/kjr6mSb6APAvgSeTfKvV/gi4FrgzyRbgO8BH2r77gIuAMeBHwEcBqupAks8Cj7Z2n6mqA330S5I0Qz2HQVX9LyDT7D5vivYFXDnNc+0AdvTaF6lXx+KekfehtBAtyncgTzc94LSBpMXKzyaSJC3OkYE0Fad3tJgZBtIUDAYtNoaBdAQGgxYD7xlIkhwZSDPhKEHDyjCQemQwaJg4TSRJcmQgDYJvWNRC58hAkuTIQDqWvK+ghcKRgSTJkYE0W/waTs1nhoE0x5xK0nxgGEjziMGguWIYSPNUv8tVDRPNxLwJgyQbgS8AS4CbquraOe6StKB5j0IzMS/CIMkS4IvAPwfGgUeT7KqqZ+a2Z9Lw6WfEYZAMr3kRBsDZwFhVvQiQ5HZgE2AYSPOIQTK85ksYrAT2dm2PA+dMbpRkK7C1bf7vJM/3+HonA9/v8diFZLGcJ3iu816um/EhC/I8ezSb5/oPpyrOlzA4KlW1Hdje7/MkGa2qkQF0aV5bLOcJnuswWiznCfPjXOfLO5D3Aau7tle1miRpFsyXMHgUWJdkbZLjgMuAXXPcJ0laNObFNFFVHUryMeB+OktLd1TV08fwJfuealogFst5guc6jBbLecI8ONdU1Vz3QZI0x+bLNJEkaQ4ZBpKkxRUGSTYmeT7JWJJtc92fQUqyOsmDSZ5J8nSST7T6iUl2J3mh/V4+130dhCRLkjye5Ktte22SPe3a3tEWIix4SZYluSvJc0meTfLrw3hNk/xh+3f7VJLbkrxjWK5pkh1J9id5qqs25TVMxw3tnJ9IctZs9XPRhEHXR15cCKwHLk+yfm57NVCHgE9W1XpgA3BlO79twANVtQ54oG0Pg08Az3ZtXwdcX1WnAweBLXPSq8H7AvD1qvoV4H10znmormmSlcDvAyNVdSadRSSXMTzX9BZg46TadNfwQmBd+9kK3DhLfVw8YUDXR15U1ZvA4Y+8GApV9UpVfbM9/ls6/2mspHOOO1uzncAlc9PDwUmyCrgYuKltBzgXuKs1GZbzfA/wG8DNAFX1ZlW9xhBeUzorG9+ZZClwAvAKQ3JNq+oh4MCk8nTXcBNwa3U8DCxLcups9HMxhcFUH3mxco76ckwlWQO8H9gDnFJVr7RdrwKnzFG3BulPgU8Bf9e2TwJeq6pDbXtYru1aYAL4szYldlOSdzFk17Sq9gGfB75LJwReBx5jOK/pYdNdwzn7f2oxhcGikOTngT8H/qCqfti9rzrriBf0WuIkHwL2V9Vjc92XWbAUOAu4sareD/wfJk0JDck1XU7nL+K1wC8C7+Jnp1WG1ny5hospDIb+Iy+S/BydIPhKVd3dyt87PMxsv/fPVf8G5APAh5O8TGeq71w68+rL2hQDDM+1HQfGq2pP276LTjgM2zX9LeClqpqoqh8Dd9O5zsN4TQ+b7hrO2f9TiykMhvojL9q8+c3As1X1J127dgGb2+PNwD2z3bdBqqqrqmpVVa2hcw2/UVW/AzwIXNqaLfjzBKiqV4G9Sd7bSufR+Vj3obqmdKaHNiQ5of07PnyeQ3dNu0x3DXcBV7RVRRuA17umk46tqlo0P8BFwN8A3wb+eK77M+Bz+6d0hppPAN9qPxfRmU9/AHgB+O/AiXPd1wGe8weBr7bHvwQ8AowB/xU4fq77N6Bz/CfAaLuufwksH8ZrCvx74DngKeDLwPHDck2B2+jcC/kxndHelumuIRA6qx6/DTxJZ4XVrPTTj6OQJC2qaSJJ0jQMA0mSYSBJMgwkSRgGkiQMA0kShoEkCfh/GBbMWyWUc60AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.hist(lengths, bins=100)\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26105"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "607"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmin(lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'DreamWorks Animation has produced some of the highest grossing animated hits of all time , such as Antz -LRB-/O1998/O-RRB- , The Prince of Egypt -LRB-/O1998/O-RRB- , Shrek -LRB-/O2001/O-RRB- , its sequels Shrek 2 -LRB-/O2004/O-RRB- , Shrek the Third -LRB-/O2007/O-RRB- and Shrek Forever After -LRB-/O2010/O-RRB- ; Shark Tale -LRB-/O2004/O-RRB- , Madagascar -LRB-/O2005/O-RRB- , Over the Hedge -LRB-/O2006/O-RRB- , Flushed Away -LRB-/O2006/O-RRB- , Bee Movie -LRB-/O2007/O-RRB- , Kung Fu Panda -LRB-/O2008/O-RRB- and How to Train Your Dragon -LRB-/O2010/O-RRB- .\\n'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ori_src[26105]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"! ''\\n\""
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ori_src[607]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_length = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = np.array([encoder(tokenizer(line), \\\n",
    "                            tokenizer_src, sentence_length) for line in \\\n",
    "                    train_ori_src]).reshape(-1, sentence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(296402, 50)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ora'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_word(3760, tokenizer_src)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Plays and comic puppet theater loosely based on this legend were popular throughout Germany in the 16th century , often reducing Faust and Mephistopheles to figures of vulgar fun .\\n']\n",
      "[['plays', 'and', 'comic', 'puppet', 'theater', 'loosely', 'based', 'on', 'this', 'legend', 'were', 'popular', 'throughout', 'germany', 'in', 'the', '16th', 'century', 'often', 'reducing', 'faust', 'and', 'mephistopheles', 'to', 'figures', 'of', 'vulgar', 'fun']]\n",
      "[[20621  8289  6051 13698  3244  3348 17759 12418 11609   837 12199 11318\n",
      "   4760 24521 18273 14198 26254   763 16891  9142  1824  8289 26254 14391\n",
      "  18683 11142 15491 16571     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0]]\n"
     ]
    }
   ],
   "source": [
    "print(train_ori_src[12:13])\n",
    "print([tokenizer(line) for line in train_ori_src[12:13]])\n",
    "print(train_x[12:13])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "lengths_dst = [len((tokenizer(x))) for x in train_ori_dst]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min 0\n",
      "mean 16.679013636885042\n",
      "max 80\n",
      "std 10.492046320896975\n"
     ]
    }
   ],
   "source": [
    "print(\"min\", np.min(lengths_dst))\n",
    "print(\"mean\", np.mean(lengths_dst))\n",
    "print(\"max\", np.max(lengths_dst))\n",
    "print(\"std\", np.std(lengths_dst))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYMAAAD4CAYAAAAO9oqkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+j8jraAAATZUlEQVR4nO3df6zddX3H8edrZaAyRws0hLVlt4sNBs1EvAGMi3HgoKCx/IGmzGjnuvWP4dTNRMuWjPmDBLNlDONkaaQKxlAQdTTAxA4hy5ZRaAERqMgdv9oGaLWAy9ic1ff+OJ+Lx+u9tPec23vO7X0+kpP7/X6+3+8573PPufd1P5/v53xvqgpJ0vz2K4MuQJI0eIaBJMkwkCQZBpIkDANJEnDEoAvo1fHHH18jIyODLkOS5pTt27f/oKoWT2yfs2EwMjLCtm3bBl2GJM0pSZ6crN1hIkmSYSBJMgwkSRgGkiQMA0kShoEkCcNAkoRhIEnCMJAkMYc/gayfG1l/y0vLT1z+jgFWImmusmcgSTIMJEmGgSQJw0CShGEgScLZRPOGM44kvZx5HwbD9kuyux4YjpokHf4cJpIkGQaSJIeJhsLEoaFDdYwkTcWegSTJMJAkGQaSJAwDSRKGgSQJw0CSxEGEQZKNSfYkebCr7W+SfC/JA0m+kWRh17ZLkowleSTJuV3tK1vbWJL1Xe3Lk2xt7dcnOXImn6Ak6cAOpmfwJWDlhLYtwOur6reB7wOXACQ5BVgNvK4d8/kkC5IsAP4BOA84Bbio7QvwGeCKqnoN8Bywtq9nJEmatgOGQVX9K7BvQtu3qmp/W70LWNqWVwGbqurHVfU4MAac3m5jVfVYVf0fsAlYlSTAWcCN7fhrgAv6fE6SpGmaiU8g/yFwfVteQiccxu1qbQA7J7SfARwHPN8VLN37/5Ik64B1ACeddFLfhc81fupY0qHSVxgk+UtgP/CVmSnn5VXVBmADwOjoaM3GY841w3YVVklzQ89hkOQPgHcCZ1fV+C/m3cCyrt2WtjamaP8hsDDJEa130L2/JGmW9DS1NMlK4GPAu6rqxa5Nm4HVSY5KshxYAdwN3AOsaDOHjqRzknlzC5E7gAvb8WuAm3p7KpKkXh3M1NLrgP8ATk6yK8la4HPAq4EtSe5P8o8AVfUQcAPwMPBN4OKq+mn7q/+DwG3ADuCGti/Ax4E/TzJG5xzC1TP6DCVJB3TAYaKqumiS5il/YVfVZcBlk7TfCtw6SftjdGYbSZIGxE8gS5IMA0mSYSBJwjCQJOH/QJ6X/GCapInsGUiSDANJkmEgScIwkCRhGEiSMAwkSTi1dGD8RzWShok9A0mSYSBJMgwkSRgGkiQMA0kShoEkCcNAkoRhIEnCMJAkYRhIkjiIMEiyMcmeJA92tR2bZEuSR9vXRa09ST6bZCzJA0lO6zpmTdv/0SRrutrflOS77ZjPJslMP0lNbWT9LS/dJM1fB9Mz+BKwckLbeuD2qloB3N7WAc4DVrTbOuAq6IQHcClwBnA6cOl4gLR9/rjruImPddjwF6+kYXXAC9VV1b8mGZnQvAp4W1u+BrgT+Hhrv7aqCrgrycIkJ7Z9t1TVPoAkW4CVSe4Efr2q7mrt1wIXAP/cz5NSb/zfyNL81es5gxOq6um2/AxwQlteAuzs2m9Xa3u59l2TtE8qybok25Js27t3b4+lS5Im6vsEcusF1AzUcjCPtaGqRqtqdPHixbPxkJI0L/QaBs+24R/a1z2tfTewrGu/pa3t5dqXTtIuSZpFvYbBZmB8RtAa4Kau9ve3WUVnAi+04aTbgHOSLGonjs8BbmvbfpTkzDaL6P1d9yVJmiUHPIGc5Do6J4CPT7KLzqygy4EbkqwFngTe03a/FTgfGANeBD4AUFX7knwKuKft98nxk8nAn9CZsfRKOieOPXksSbPsYGYTXTTFprMn2beAi6e4n43AxknatwGvP1AdGhxnGUmHPz+BLEk6cM9A0+df0pLmGsNA02LQSYcnh4kkSYaBJMkwkCRhGEiSMAwkSRgGkiQMA0kShoEkCcNAkoRhIEnCMJAkYRhIkjAMJEkYBpIkDANJEoaBJAnDQJKEYSBJwjCQJNFnGCT5syQPJXkwyXVJXpFkeZKtScaSXJ/kyLbvUW19rG0f6bqfS1r7I0nO7e8pSZKmq+cwSLIE+BAwWlWvBxYAq4HPAFdU1WuA54C17ZC1wHOt/Yq2H0lOace9DlgJfD7Jgl7rkiRNX7/DREcAr0xyBPAq4GngLODGtv0a4IK2vKqt07afnSStfVNV/biqHgfGgNP7rEuSNA1H9HpgVe1O8rfAU8D/AN8CtgPPV9X+ttsuYElbXgLsbMfuT/ICcFxrv6vrrruP+QVJ1gHrAE466aReSz8kRtbfMugSZl33c37i8ncMsBJJ/eo5DJIsovNX/XLgeeCrdIZ5Dpmq2gBsABgdHa2Zvn9/uUmar/oZJno78HhV7a2qnwBfB94CLGzDRgBLgd1teTewDKBtPwb4YXf7JMdIkmZBP2HwFHBmkle1sf+zgYeBO4AL2z5rgJva8ua2Ttv+7aqq1r66zTZaDqwA7u6jLknSNPVzzmBrkhuBe4H9wH10hnBuATYl+XRru7odcjXw5SRjwD46M4ioqoeS3EAnSPYDF1fVT3utS5I0fT2HAUBVXQpcOqH5MSaZDVRV/wu8e4r7uQy4rJ9aJEm98xPIkiTDQJJkGEiSMAwkSRgGkiQMA0kSfU4tlcZ5KQ9pbrNnIEkyDCRJhoEkCcNAkoRhIEnCMJAkYRhIkjAMJEkYBpIkDANJEl6OQoeYl6mQ5gZ7BpIkewaaPfYSpOFlz0CSZBhIkvoMgyQLk9yY5HtJdiR5c5Jjk2xJ8mj7uqjtmySfTTKW5IEkp3Xdz5q2/6NJ1vT7pCRJ09Nvz+BK4JtV9VrgDcAOYD1we1WtAG5v6wDnASvabR1wFUCSY4FLgTOA04FLxwNEkjQ7eg6DJMcAbwWuBqiq/6uq54FVwDVtt2uAC9ryKuDa6rgLWJjkROBcYEtV7auq54AtwMpe65IkTV8/PYPlwF7gi0nuS/KFJEcDJ1TV022fZ4AT2vISYGfX8bta21TtvyTJuiTbkmzbu3dvH6VLkrr1EwZHAKcBV1XVG4H/5udDQgBUVQHVx2P8gqraUFWjVTW6ePHimbpbSZr3+vmcwS5gV1Vtbes30gmDZ5OcWFVPt2GgPW37bmBZ1/FLW9tu4G0T2u/so65Z0z1vXpLmsp57BlX1DLAzycmt6WzgYWAzMD4jaA1wU1veDLy/zSo6E3ihDSfdBpyTZFE7cXxOa5MkzZJ+P4H8p8BXkhwJPAZ8gE7A3JBkLfAk8J62763A+cAY8GLbl6ral+RTwD1tv09W1b4+65IkTUNfYVBV9wOjk2w6e5J9C7h4ivvZCGzspxZJUu+8NpEGwusUScPFy1FIkuwZTJcziCQdjuwZSJIMA0mSYSBJwjCQJGEYSJJwNtGUnAcvaT6xZyBJMgwkSQ4TaQg4JCcNnj0DSZJhIEkyDCRJGAaSJAwDSRLOJtIQc5aRNHvsGUiSDANJkmEgScIwkCQxA2GQZEGS+5Lc3NaXJ9maZCzJ9UmObO1HtfWxtn2k6z4uae2PJDm335okSdMzEz2DDwM7utY/A1xRVa8BngPWtva1wHOt/Yq2H0lOAVYDrwNWAp9PsmAG6pIkHaS+ppYmWQq8A7gM+PMkAc4Cfr/tcg3w18BVwKq2DHAj8Lm2/ypgU1X9GHg8yRhwOvAf/dSmw0v3NFNwqqk00/rtGfw98DHgZ239OOD5qtrf1ncBS9ryEmAnQNv+Qtv/pfZJjpEkzYKewyDJO4E9VbV9Bus50GOuS7Ityba9e/fO1sNK0mGvn57BW4B3JXkC2ERneOhKYGGS8eGnpcDutrwbWAbQth8D/LC7fZJjfkFVbaiq0aoaXbx4cR+lS5K69RwGVXVJVS2tqhE6J4C/XVXvBe4ALmy7rQFuasub2zpt+7erqlr76jbbaDmwAri717okSdN3KK5N9HFgU5JPA/cBV7f2q4EvtxPE++gECFX1UJIbgIeB/cDFVfXTQ1CXDiNet0iaWTMSBlV1J3BnW36Mzmygifv8L/DuKY6/jM6MJEnSAPgJZEmSYSBJMgwkSRgGkiQMA0kS/ttLHQacZir1z56BJMkwkCQZBpIkPGegw5jnEqSDZ89AkmQYSJIMA0kShoEkCU8ga57wZLL08uwZSJIMA0mSYSBJwjCQJGEYSJIwDCRJOLVU85DTTKVfZs9AktR7GCRZluSOJA8neSjJh1v7sUm2JHm0fV3U2pPks0nGkjyQ5LSu+1rT9n80yZr+n5YkaTr6GSbaD3y0qu5N8mpge5ItwB8At1fV5UnWA+uBjwPnASva7QzgKuCMJMcClwKjQLX72VxVz/VRm3RQHDKSOnruGVTV01V1b1v+L2AHsARYBVzTdrsGuKAtrwKurY67gIVJTgTOBbZU1b4WAFuAlb3WJUmavhk5Z5BkBHgjsBU4oaqebpueAU5oy0uAnV2H7WptU7VP9jjrkmxLsm3v3r0zUbokiRmYTZTk14CvAR+pqh8leWlbVVWS6vcxuu5vA7ABYHR0dMbuV5rI4SPNN331DJL8Kp0g+EpVfb01P9uGf2hf97T23cCyrsOXtrap2iVJs6Sf2UQBrgZ2VNXfdW3aDIzPCFoD3NTV/v42q+hM4IU2nHQbcE6SRW3m0TmtTRoKI+tveekmHa76GSZ6C/A+4LtJ7m9tfwFcDtyQZC3wJPCetu1W4HxgDHgR+ABAVe1L8ingnrbfJ6tqXx91SZKmqecwqKp/AzLF5rMn2b+Ai6e4r43Axl5rkST1x08gS5K8NpE0Hc4y0uHKnoEkyZ6B1Ct7CTqc2DOQJBkGkiSHiaQZ5/CR5iJ7BpIkewbSoWQvQXOFPQNJkmEgSXKYSJo1DhlpmNkzkCTZM5AGwV6Cho1hIA0RQ0KDYhhIQ8pg0GwyDKQ5wGDQoeYJZEmSPQNprnm5XoI9CPXKnoEkyZ6BdLjq7iVMxd6Dxs3LMDiYHxJpvpnq58LAmB+GJgySrASuBBYAX6iqywdckiQOLiQ8VzH3DUUYJFkA/APwe8Au4J4km6vq4cFWJmm6pjs8ZZAMh6EIA+B0YKyqHgNIsglYBRgG0jzST5AcrOkeP18CKlU16BpIciGwsqr+qK2/Dzijqj44Yb91wLq2ejLwSI8PeTzwgx6PPZSsa3qsa3qsa3oO17p+s6oWT2wclp7BQamqDcCGfu8nybaqGp2BkmaUdU2PdU2PdU3PfKtrWD5nsBtY1rW+tLVJkmbBsITBPcCKJMuTHAmsBjYPuCZJmjeGYpioqvYn+SBwG52ppRur6qFD+JB9DzUdItY1PdY1PdY1PfOqrqE4gSxJGqxhGSaSJA2QYSBJml9hkGRlkkeSjCVZP+BaNibZk+TBrrZjk2xJ8mj7umiWa1qW5I4kDyd5KMmHh6GuVsMrktyd5Duttk+09uVJtrbX9Po2AWG2a1uQ5L4kNw9LTa2OJ5J8N8n9Sba1tmF4LRcmuTHJ95LsSPLmQdeV5OT2fRq//SjJRwZdV6vtz9p7/sEk17WfhRl/j82bMOi65MV5wCnARUlOGWBJXwJWTmhbD9xeVSuA29v6bNoPfLSqTgHOBC5u36NB1wXwY+CsqnoDcCqwMsmZwGeAK6rqNcBzwNoB1PZhYEfX+jDUNO53q+rUrnnpw/BaXgl8s6peC7yBzvduoHVV1SPt+3Qq8CbgReAbg64ryRLgQ8BoVb2ezgSb1RyK91hVzYsb8Gbgtq71S4BLBlzTCPBg1/ojwIlt+UTgkQHXdxOd60UNW12vAu4FzqDzScwjJnuNZ6mWpXR+SZwF3Axk0DV11fYEcPyEtoG+lsAxwOO0ySvDUteEWs4B/n0Y6gKWADuBY+nM/rwZOPdQvMfmTc+An39Tx+1qbcPkhKp6ui0/A5wwqEKSjABvBLYyJHW14Zj7gT3AFuA/geeran/bZRCv6d8DHwN+1taPG4KaxhXwrSTb26VcYPCv5XJgL/DFNrT2hSRHD0Fd3VYD17XlgdZVVbuBvwWeAp4GXgC2cwjeY/MpDOaU6kT+QOb9Jvk14GvAR6rqR8NSV1X9tDrd+KV0Lm742kHUMS7JO4E9VbV9kHW8jN+pqtPoDI1enOSt3RsH9FoeAZwGXFVVbwT+mwlDLwN+7x8JvAv46sRtg6irnaNYRSdEfwM4ml8eXp4R8ykM5sIlL55NciJA+7pntgtI8qt0guArVfX1YamrW1U9D9xBp3u8MMn4hydn+zV9C/CuJE8Am+gMFV054Jpe0v6qpKr20Bn/Pp3Bv5a7gF1VtbWt30gnHAZd17jzgHur6tm2Pui63g48XlV7q+onwNfpvO9m/D02n8JgLlzyYjOwpi2voTNmP2uSBLga2FFVfzcsdbXaFidZ2JZfSedcxg46oXDhIGqrqkuqamlVjdB5P327qt47yJrGJTk6yavHl+mMgz/IgF/LqnoG2Jnk5NZ0Np1L1Q/8PdZcxM+HiGDwdT0FnJnkVe3nc/z7NfPvsUGdpBnEDTgf+D6dsea/HHAt19EZA/wJnb+W1tIZb74deBT4F+DYWa7pd+h0gx8A7m+38wddV6vtt4H7Wm0PAn/V2n8LuBsYo9O1P2pAr+fbgJuHpaZWw3fa7aHx9/uQvJanAtvaa/lPwKIhqeto4IfAMV1tw1DXJ4Dvtff9l4GjDsV7zMtRSJLm1TCRJGkKhoEkyTCQJBkGkiQMA0kShoEkCcNAkgT8P42GqkXB6WdiAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.hist(lengths_dst, bins=100)\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_length_dst = 40"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Look at output vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20570\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('the', 300506),\n",
       " ('of', 169568),\n",
       " ('in', 134603),\n",
       " ('a', 119036),\n",
       " ('and', 114305),\n",
       " ('is', 109630),\n",
       " ('to', 69806),\n",
       " ('was', 54588),\n",
       " ('for', 31547),\n",
       " ('by', 29698)]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_list_dst_over_threshold = build_vocabulary(tokenize(clean(train_ori_dst)), threshold=10)\n",
    "vocabulary_size_dst = len(token_list_dst_over_threshold) + 1\n",
    "print(vocabulary_size_dst)\n",
    "token_list_dst_over_threshold[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19665\n"
     ]
    }
   ],
   "source": [
    "improved_token_list_dst = improve_token_list([x[0] for x in token_list_dst_over_threshold], \\\n",
    "                                         embedding_index, separate_words)\n",
    "improved_token_list_dst = list((set(improved_token_list_dst)))\n",
    "vocabulary_size_dst = len(improved_token_list_dst) + 1\n",
    "print(vocabulary_size_dst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19665"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_dst = build_word2index(improved_token_list_dst)\n",
    "len(tokenizer_dst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y = np.array([encoder(tokenizer(line), \\\n",
    "                            tokenizer_dst, sentence_length_dst) for line in \\\n",
    "                    train_ori_dst]).reshape(-1, sentence_length_dst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(296402, 40)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9279"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0 in tokenizer_src.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import Callback\n",
    "class NeptuneMonitor(Callback):\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        my_metric = logs['loss']\n",
    "        neptune.send_metric('Sparse Categorical Cross Entropy', epoch, my_metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix.shape[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod = build_model(vocabulary_size_source, vocabulary_size_dst, sentence_length, \\\n",
    "                  sentence_length_dst, 100, embedding_matrix, unfreeze_emb=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod = build_model(vocabulary_size_source, vocabulary_size_dst, sentence_length, \\\n",
    "                  sentence_length_dst, 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 50)]              0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 50, 32)            840192    \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 32)                8320      \n",
      "_________________________________________________________________\n",
      "repeat_vector_1 (RepeatVecto (None, 40, 32)            0         \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 40, 32)            8320      \n",
      "_________________________________________________________________\n",
      "time_distributed_1 (TimeDist (None, 40, 19666)         648978    \n",
      "=================================================================\n",
      "Total params: 1,505,810\n",
      "Trainable params: 1,505,810\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "mod.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(296402, 40)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.6552\n",
      "Epoch 2/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.6573\n",
      "Epoch 3/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.6837\n",
      "Epoch 4/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.6531\n",
      "Epoch 5/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.6395\n",
      "Epoch 6/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.6472\n",
      "Epoch 7/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.6796\n",
      "Epoch 8/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.6358\n",
      "Epoch 9/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.6225\n",
      "Epoch 10/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.6202\n",
      "Epoch 11/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.6420\n",
      "Epoch 12/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.6387\n",
      "Epoch 13/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.6629\n",
      "Epoch 14/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.6270\n",
      "Epoch 15/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.6205\n",
      "Epoch 16/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.6078\n",
      "Epoch 17/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.6278\n",
      "Epoch 18/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.6115\n",
      "Epoch 19/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.6159\n",
      "Epoch 20/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.6037\n",
      "Epoch 21/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.6330\n",
      "Epoch 22/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.6227\n",
      "Epoch 23/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.6204\n",
      "Epoch 24/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.5898\n",
      "Epoch 25/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.6042\n",
      "Epoch 26/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.6030\n",
      "Epoch 27/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.6240\n",
      "Epoch 28/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.6073\n",
      "Epoch 29/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.5939\n",
      "Epoch 30/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.5856\n",
      "Epoch 31/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.6023\n",
      "Epoch 32/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.5929\n",
      "Epoch 33/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.5967\n",
      "Epoch 34/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.5910\n",
      "Epoch 35/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.5935\n",
      "Epoch 36/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.5648\n",
      "Epoch 37/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.5694\n",
      "Epoch 38/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.5742\n",
      "Epoch 39/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.5992\n",
      "Epoch 40/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.5695\n",
      "Epoch 41/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.5599\n",
      "Epoch 42/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.5659\n",
      "Epoch 43/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.5903\n",
      "Epoch 44/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.5608\n",
      "Epoch 45/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.5543\n",
      "Epoch 46/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.5545\n",
      "Epoch 47/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.5661\n",
      "Epoch 48/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.5676\n",
      "Epoch 49/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.5662\n",
      "Epoch 50/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.5659\n",
      "Epoch 51/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.5680\n",
      "Epoch 52/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.5290\n",
      "Epoch 53/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.5215\n",
      "Epoch 54/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.5288\n",
      "Epoch 55/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.5576\n",
      "Epoch 56/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.5454\n",
      "Epoch 57/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.5541\n",
      "Epoch 58/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.5407\n",
      "Epoch 59/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.5480\n",
      "Epoch 60/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.5271\n",
      "Epoch 61/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.5243\n",
      "Epoch 62/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.5291\n",
      "Epoch 63/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.5389\n",
      "Epoch 64/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.5263\n",
      "Epoch 65/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.5268\n",
      "Epoch 66/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.5330\n",
      "Epoch 67/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.5359\n",
      "Epoch 68/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.5042\n",
      "Epoch 69/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.5004\n",
      "Epoch 70/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.5131\n",
      "Epoch 71/100\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.5419\n",
      "Epoch 72/100\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# import datetime\n",
    "# import neptune\n",
    "# #from simplifier import generator\n",
    "# #import tensorboard\n",
    "\n",
    "# #PARAMS = {'n_iterations': 117,\n",
    "# #          'n_images': 5}\n",
    "\n",
    "# # neptune.init('l-theorist/sandbox',\n",
    "# #              api_token='eyJhcGlfYWRkcmVzcyI6Imh0dHBzOi8vdWkubmVwdHVuZS5haSIsImFwaV91cmwiOiJodHRwczovL3VpLm5lcHR1bmUuYWkiLCJhcGlfa2V5IjoiNDYyODYzNzAtNWMzOC00ZTY5LWJkMjYtYTRhNTVkNWM4MWZjIn0=')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# rms = optimizers.RMSprop(lr=0.001)\n",
    "# nadm =optimizers.Nadam(learning_rate=0.008, beta_1=0.99, beta_2=0.99999)\n",
    "# admx = optimizers.Adamax(learning_rate=0.0006, beta_1=0.999, beta_2=0.999999)\n",
    "# mod.compile(\n",
    "#     optimizer=rms,\n",
    "#     #optimizer=\"adam\",\n",
    "#     #loss=\"categorical_crossentropy\",\n",
    "#     loss=\"sparse_categorical_crossentropy\",\n",
    "#     #metrics=[\"accuracy\"]\n",
    "\n",
    "# )\n",
    "\n",
    "\n",
    "# with neptune.create_experiment(name='baseline simplifier',\n",
    "#                                #params=PARAMS\n",
    "#                               ):\n",
    "#     neptune_monitor = NeptuneMonitor()\n",
    "   \n",
    "\n",
    "batch_size = 64\n",
    "n_samples = 64 #train_x.shape[0]\n",
    "history = mod.fit_generator(\n",
    "    generator(train_x, train_y, batch_size, samples=n_samples, shf=False, onehot=False),\n",
    "    #data_gen,\n",
    "    #(train_gen(train_x, 500), one_hot_encode_output(train_y_gen)),\n",
    "    epochs=100,\n",
    "    #batch_size=50,\n",
    "    #validation_split=0.2,\n",
    "    #steps_per_epoch = train_x.shape[0]/batch_size,\n",
    "    steps_per_epoch = n_samples/batch_size,\n",
    "  #  callbacks=[neptune_monitor]\n",
    "    \n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Model' object has no attribute 'predict_classes'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-162-363b926d035f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmod\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_classes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'Model' object has no attribute 'predict_classes'"
     ]
    }
   ],
   "source": [
    "mod.predict_classes(train_x[10:12])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tensorflow.python.keras.engine.input_layer.InputLayer at 0x7f9b080a0ac8>,\n",
       " <tensorflow.python.keras.layers.embeddings.Embedding at 0x7f9b080a0ef0>,\n",
       " <tensorflow.python.keras.layers.recurrent.LSTM at 0x7f9b21629048>,\n",
       " <tensorflow.python.keras.layers.core.RepeatVector at 0x7f9b07cfa0f0>,\n",
       " <tensorflow.python.keras.layers.recurrent.LSTM at 0x7f9b07d019b0>,\n",
       " <tensorflow.python.keras.layers.wrappers.TimeDistributed at 0x7f9b07cc9eb8>]"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mod.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from kerastuner import HyperModel\n",
    "\n",
    "\n",
    "class SimpleHyperModel(HyperModel):\n",
    "    def __init__(self, vocab_in, vocab_out, length_in, length_out, n_units, \\\n",
    "                    use_emb=None, unfreeze_emb=True):\n",
    "        self.vocab_in = vocab_in\n",
    "        self.vocab_out = vocab_out\n",
    "        self.length_in = length_in\n",
    "        self.length_out = length_out\n",
    "        self.n_units = n_units\n",
    "        self.use_emb = use_emb\n",
    "        self.unfreeze_emb = unfreeze_emb\n",
    "        \n",
    "\n",
    "    def build(self, hp):\n",
    "        \n",
    "        if self.use_emb is not None:\n",
    "            assert self.n_units==self.use_emb.shape[-1], \"Embedding dimension should match n_units.\"\n",
    "        encoder_input = Input(shape=(self.length_in,))\n",
    "        encoder_output = layers.Embedding(self.vocab_in+1,\n",
    "                                        self.n_units,\n",
    "                                        input_length=self.length_in,\n",
    "                                        embeddings_initializer='lecun_uniform',\n",
    "                                        mask_zero=True,\n",
    "                                        trainable=True)(encoder_input)\n",
    "        encoder_output = layers.LSTM(self.n_units)(encoder_output)\n",
    "        encoder_output = layers.RepeatVector(self.length_out)(encoder_output)\n",
    "        decoder_output = layers.LSTM(self.n_units, return_sequences=True)(encoder_output)\n",
    "        decoder_output = layers.TimeDistributed(layers.Dense(self.vocab_out+1, activation='softmax'))(decoder_output)\n",
    "\n",
    "        model = models.Model(encoder_input, decoder_output)\n",
    "        if self.use_emb is not None:\n",
    "            model.layers[1].set_weights([self.use_emb])\n",
    "            model.layers[1].trainable = self.unfreeze_emb\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "        model.compile(\n",
    "            optimizer=keras.optimizers.Adam(\n",
    "                hp.Float(\n",
    "                    'learning_rate',\n",
    "                    min_value=1e-4,\n",
    "                    max_value=1e-2,\n",
    "                    sampling='LOG',\n",
    "                    default=1e-3\n",
    "                )\n",
    "            ),\n",
    "            loss='sparse_categorical_crossentropy',\n",
    "            #metrics=['accuracy']\n",
    "        )\n",
    "        return model\n",
    "\n",
    "hypermodel = SimpleHyperModel(vocabulary_size_source, vocabulary_size_dst, sentence_length, \\\n",
    "                  sentence_length_dst, 512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kerastuner.tuners import RandomSearch #,Hyperband\n",
    "HYPERBAND_MAX_EPOCHS=20\n",
    "SEED=17\n",
    "MAX_TRIALS = 10\n",
    "EXECUTION_PER_TRIAL = 2\n",
    "\n",
    "\n",
    "\n",
    "tuner = RandomSearch(\n",
    "    hypermodel,\n",
    "    #max_epochs=HYPERBAND_MAX_EPOCHS,\n",
    "    objective='loss',\n",
    "    max_trials=MAX_TRIALS,\n",
    "    seed=SEED,\n",
    "    executions_per_trial=EXECUTION_PER_TRIAL,\n",
    "    directory='randomsearch',\n",
    "    project_name='fil'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span style=\"color:#4527A0\"><h1 style=\"font-size:18px\">Search space summary</h1></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Default search space size: 1</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#7E57C2\"><h2 style=\"font-size:16px\">learning_rate (Float)</h2></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-default: 0.001</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-max_value: 0.01</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-min_value: 0.0001</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-sampling: log</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-step: None</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tuner.search_space_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'RandomSearch' object has no attribute 'search_generator'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-159-26fd0beb5f97>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m128\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mn_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m256\u001b[0m \u001b[0;31m#train_x.shape[0]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mtuner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msearch_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msamples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0monehot\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mN_EPOCH_SEARCH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'RandomSearch' object has no attribute 'search_generator'"
     ]
    }
   ],
   "source": [
    "N_EPOCH_SEARCH = 20\n",
    "batch_size = 128\n",
    "n_samples = 256 #train_x.shape[0]\n",
    "tuner.search_generator(generator(train_x, train_y, batch_size, samples=n_samples, shf=False, onehot=False), epochs=N_EPOCH_SEARCH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(model.history.history[\"loss\"], label=\"loss\")\n",
    "#plt.plot(model.history.history[\"val_loss\"], label=\"validation loss\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict(train_x[10:11]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ori_src[10:11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x[10:11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict_classes(train_x[10:11])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['                                                 ']"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convert(model.predict_classes(train_x[10:11]), token_dict=tokenizer_dst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['many still refer to <UNK>  and <UNK>  as <UNK>  and <UNK>  respectively not unlike the usage of bit in american english for a                       ']"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convert(train_x[10:11], tokenizer_src)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ori_dst[10:11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "model = load_model(\"model_all_1.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(model.get_weights()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "mdl = load_model(\"first_3K_smod.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(mdl.get_weights()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x[indx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y[indx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sequence(model, tokenizer, source):\n",
    "    prediction = model.predict(source, verbose=0)\n",
    "    integers = [np.argmax(vector) for vector in prediction]\n",
    "    return convert(integers, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_sequence(model, tokenizer_dst, train_x[10:11])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = model.predict(train_x[indx], verbose=0)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "integers = [np.argmax(vector) for vector in prediction]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "simple-simplifier",
   "language": "python",
   "name": "simple-simplifier"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
