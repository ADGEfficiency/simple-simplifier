{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to /home/filipp/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import pathlib\n",
    "import nltk\n",
    "import numpy as np\n",
    "from unicodedata import normalize\n",
    "import collections\n",
    "\n",
    "\n",
    "nltk.download('words')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_data = 'data-simplification/wikilarge/wiki.full.aner.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_src = open(path_data+'train.src', \"r\").readlines()\n",
    "train_dst = open(path_data+'train.dst', \"r\").readlines()\n",
    "train_ori_src = open(path_data+'ori.train.src', \"r\").readlines()\n",
    "train_ori_dst = open(path_data+'ori.train.dst', \"r\").readlines()\n",
    "\n",
    "valid_src = open(path_data+'valid.src', \"r\").readlines()\n",
    "valid_dst = open(path_data+'valid.dst', \"r\").readlines()\n",
    "valid_ori_src = open(path_data+'ori.valid.src', \"r\").readlines()\n",
    "valid_ori_dst = open(path_data+'ori.valid.dst', \"r\").readlines()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison of datasets; will work with original first. Named entity identification is applied to get the modified dataset. Might facilitate trainining working with this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ORGANIZATION@1 crews conducted backburning operations to ensure containment of the fire on NUMBER@1 February , warning residents of areas between LOCATION@1 and Warragul about smoke from those fires .\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_src[114]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'DSE crews conducted backburning operations to ensure containment of the fire on 9 February , warning residents of areas between Pakenham and Warragul about smoke from those fires .\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ori_src[114]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'People between Pakenham and Warragul were warned about smoke from those fires .\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ori_dst[114]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'People between LOCATION@1 and Warragul were warned about smoke from those fires .\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dst[114]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Thousands of young people from all over the world went to San Francisco to help create a hippie counterculture . The Summer of Love made the rest of America much more aware of the hippie movement .\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_ori_dst[11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ie_preprocess(document):\n",
    "    sentences = nltk.sent_tokenize(document) \n",
    "    sentences = [nltk.word_tokenize(sent) for sent in sentences] \n",
    "    sentences = [nltk.pos_tag(sent) for sent in sentences] \n",
    "    return sentences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_index = {}\n",
    "file = os.path.join(str(pathlib.Path.home())+\"/GloVe\",\"glove.6B.100d.txt\")\n",
    "f= open(file)\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype=\"float32\")\n",
    "    embedding_index[word] = coefs\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A glimpse at the representations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the [-0.038194 -0.24487   0.72812  -0.39961   0.083172  0.043953 -0.39141\n",
      "  0.3344   -0.57545   0.087459  0.28787  -0.06731   0.30906  -0.26384\n",
      " -0.13231  -0.20757   0.33395  -0.33848  -0.31743  -0.48336   0.1464\n",
      " -0.37304   0.34577   0.052041  0.44946  -0.46971   0.02628  -0.54155\n",
      " -0.15518  -0.14107  -0.039722  0.28277   0.14393   0.23464  -0.31021\n",
      "  0.086173  0.20397   0.52624   0.17164  -0.082378 -0.71787  -0.41531\n",
      "  0.20335  -0.12763   0.41367   0.55187   0.57908  -0.33477  -0.36559\n",
      " -0.54857  -0.062892  0.26584   0.30205   0.99775  -0.80481  -3.0243\n",
      "  0.01254  -0.36942   2.2167    0.72201  -0.24978   0.92136   0.034514\n",
      "  0.46745   1.1079   -0.19358  -0.074575  0.23353  -0.052062 -0.22044\n",
      "  0.057162 -0.15806  -0.30798  -0.41625   0.37972   0.15006  -0.53212\n",
      " -0.2055   -1.2526    0.071624  0.70565   0.49744  -0.42063   0.26148\n",
      " -1.538    -0.30223  -0.073438 -0.28312   0.37104  -0.25217   0.016215\n",
      " -0.017099 -0.38984   0.87424  -0.72569  -0.51058  -0.52028  -0.1459\n",
      "  0.8278    0.27062 ]\n",
      "(100,)\n",
      ", [-0.10767    0.11053    0.59812   -0.54361    0.67396    0.10663\n",
      "  0.038867   0.35481    0.06351   -0.094189   0.15786   -0.81665\n",
      "  0.14172    0.21939    0.58505   -0.52158    0.22783   -0.16642\n",
      " -0.68228    0.3587     0.42568    0.19021    0.91963    0.57555\n",
      "  0.46185    0.42363   -0.095399  -0.42749   -0.16567   -0.056842\n",
      " -0.29595    0.26037   -0.26606   -0.070404  -0.27662    0.15821\n",
      "  0.69825    0.43081    0.27952   -0.45437   -0.33801   -0.58184\n",
      "  0.22364   -0.5778    -0.26862   -0.20425    0.56394   -0.58524\n",
      " -0.14365   -0.64218    0.0054697 -0.35248    0.16162    1.1796\n",
      " -0.47674   -2.7553    -0.1321    -0.047729   1.0655     1.1034\n",
      " -0.2208     0.18669    0.13177    0.15117    0.7131    -0.35215\n",
      "  0.91348    0.61783    0.70992    0.23955   -0.14571   -0.37859\n",
      " -0.045959  -0.47368    0.2385     0.20536   -0.18996    0.32507\n",
      " -1.1112    -0.36341    0.98679   -0.084776  -0.54008    0.11726\n",
      " -1.0194    -0.24424    0.12771    0.013884   0.080374  -0.35414\n",
      "  0.34951   -0.7226     0.37549    0.4441    -0.99059    0.61214\n",
      " -0.35111   -0.83155    0.45293    0.082577 ]\n",
      "(100,)\n",
      ". [-0.33979    0.20941    0.46348   -0.64792   -0.38377    0.038034\n",
      "  0.17127    0.15978    0.46619   -0.019169   0.41479   -0.34349\n",
      "  0.26872    0.04464    0.42131   -0.41032    0.15459    0.022239\n",
      " -0.64653    0.25256    0.043136  -0.19445    0.46516    0.45651\n",
      "  0.68588    0.091295   0.21875   -0.70351    0.16785   -0.35079\n",
      " -0.12634    0.66384   -0.2582     0.036542  -0.13605    0.40253\n",
      "  0.14289    0.38132   -0.12283   -0.45886   -0.25282   -0.30432\n",
      " -0.11215   -0.26182   -0.22482   -0.44554    0.2991    -0.85612\n",
      " -0.14503   -0.49086    0.0082973 -0.17491    0.27524    1.4401\n",
      " -0.21239   -2.8435    -0.27958   -0.45722    1.6386     0.78808\n",
      " -0.55262    0.65       0.086426   0.39012    1.0632    -0.35379\n",
      "  0.48328    0.346      0.84174    0.098707  -0.24213   -0.27053\n",
      "  0.045287  -0.40147    0.11395    0.0062226  0.036673   0.018518\n",
      " -1.0213    -0.20806    0.64072   -0.068763  -0.58635    0.33476\n",
      " -1.1432    -0.1148    -0.25091   -0.45907   -0.096819  -0.17946\n",
      " -0.063351  -0.67412   -0.068895   0.53604   -0.87773    0.31802\n",
      " -0.39242   -0.23394    0.47298   -0.028803 ]\n",
      "(100,)\n",
      "of [-0.1529   -0.24279   0.89837   0.16996   0.53516   0.48784  -0.58826\n",
      " -0.17982  -1.3581    0.42541   0.15377   0.24215   0.13474   0.41193\n",
      "  0.67043  -0.56418   0.42985  -0.012183 -0.11677   0.31781   0.054177\n",
      " -0.054273  0.35516  -0.30241   0.31434  -0.33846   0.71715  -0.26855\n",
      " -0.15837  -0.47467   0.051581 -0.33252   0.15003  -0.1299   -0.54617\n",
      " -0.37843   0.64261   0.82187  -0.080006  0.078479 -0.96976  -0.57741\n",
      "  0.56491  -0.39873  -0.057099  0.19743   0.065706 -0.48092  -0.20125\n",
      " -0.40834   0.39456  -0.02642  -0.11838   1.012    -0.53171  -2.7474\n",
      " -0.042981 -0.74849   1.7574    0.59085   0.04885   0.78267   0.38497\n",
      "  0.42097   0.67882   0.10337   0.6328   -0.026595  0.58647  -0.44332\n",
      "  0.33057  -0.12022  -0.55645   0.073611  0.20915   0.43395  -0.012761\n",
      "  0.089874 -1.7991    0.084808  0.77112   0.63105  -0.90685   0.60326\n",
      " -1.7515    0.18596  -0.50687  -0.70203   0.66578  -0.81304   0.18712\n",
      " -0.018488 -0.26757   0.727    -0.59363  -0.34839  -0.56094  -0.591\n",
      "  1.0039    0.20664 ]\n",
      "(100,)\n",
      "to [-1.8970e-01  5.0024e-02  1.9084e-01 -4.9184e-02 -8.9737e-02  2.1006e-01\n",
      " -5.4952e-01  9.8377e-02 -2.0135e-01  3.4241e-01 -9.2677e-02  1.6100e-01\n",
      " -1.3268e-01 -2.8160e-01  1.8737e-01 -4.2959e-01  9.6039e-01  1.3972e-01\n",
      " -1.0781e+00  4.0518e-01  5.0539e-01 -5.5064e-01  4.8440e-01  3.8044e-01\n",
      " -2.9055e-03 -3.4942e-01 -9.9696e-02 -7.8368e-01  1.0363e+00 -2.3140e-01\n",
      " -4.7121e-01  5.7126e-01 -2.1454e-01  3.5958e-01 -4.8319e-01  1.0875e+00\n",
      "  2.8524e-01  1.2447e-01 -3.9248e-02 -7.6732e-02 -7.6343e-01 -3.2409e-01\n",
      " -5.7490e-01 -1.0893e+00 -4.1811e-01  4.5120e-01  1.2112e-01 -5.1367e-01\n",
      " -1.3349e-01 -1.1378e+00 -2.8768e-01  1.6774e-01  5.5804e-01  1.5387e+00\n",
      "  1.8859e-02 -2.9721e+00 -2.4216e-01 -9.2495e-01  2.1992e+00  2.8234e-01\n",
      " -3.4780e-01  5.1621e-01 -4.3387e-01  3.6852e-01  7.4573e-01  7.2102e-02\n",
      "  2.7931e-01  9.2569e-01 -5.0336e-02 -8.5856e-01 -1.3580e-01 -9.2551e-01\n",
      " -3.3991e-01 -1.0394e+00 -6.7203e-02 -2.1379e-01 -4.7690e-01  2.1377e-01\n",
      " -8.4008e-01  5.2536e-02  5.9298e-01  2.9604e-01 -6.7644e-01  1.3916e-01\n",
      " -1.5504e+00 -2.0765e-01  7.2220e-01  5.2056e-01 -7.6221e-02 -1.5194e-01\n",
      " -1.3134e-01  5.8617e-02 -3.1869e-01 -6.1419e-01 -6.2393e-01 -4.1548e-01\n",
      " -3.8175e-02 -3.9804e-01  4.7647e-01 -1.5983e-01]\n",
      "(100,)\n",
      "and [-0.071953  0.23127   0.023731 -0.50638   0.33923   0.1959   -0.32943\n",
      "  0.18364  -0.18057   0.28963   0.20448  -0.5496    0.27399   0.58327\n",
      "  0.20468  -0.49228   0.19974  -0.070237 -0.88049   0.29485   0.14071\n",
      " -0.1009    0.99449   0.36973   0.44554   0.28998  -0.1376   -0.56365\n",
      " -0.029365 -0.4122   -0.25269   0.63181  -0.44767   0.24363  -0.10813\n",
      "  0.25164   0.46967   0.3755   -0.23613  -0.14129  -0.44537  -0.65737\n",
      " -0.042421 -0.28636  -0.28811   0.063766  0.20281  -0.53542   0.41307\n",
      " -0.59722  -0.38614   0.19389  -0.17809   1.6618   -0.011819 -2.3737\n",
      "  0.058427 -0.2698    1.2823    0.81925  -0.22322   0.72932  -0.053211\n",
      "  0.43507   0.85011  -0.42935   0.92664   0.39051   1.0585   -0.24561\n",
      " -0.18265  -0.5328    0.059518 -0.66019   0.18991   0.28836  -0.2434\n",
      "  0.52784  -0.65762  -0.14081   1.0491    0.5134   -0.23816   0.69895\n",
      " -1.4813   -0.2487   -0.17936  -0.059137 -0.08056  -0.48782   0.014487\n",
      " -0.6259   -0.32367   0.41862  -1.0807    0.46742  -0.49931  -0.71895\n",
      "  0.86894   0.19539 ]\n",
      "(100,)\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "for key, value in embedding_index.items():\n",
    "    if i>5:\n",
    "        break\n",
    "    print(key, value)\n",
    "    print(value.shape)\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(sentences): \n",
    "    clean_txt = ''\n",
    "    for line in sentences:        \n",
    "\n",
    "        line = normalize('NFD', line).encode('ascii', 'ignore')\n",
    "        line = line.decode('UTF-8')\n",
    "        # tokenize on white space\n",
    "        line = line.split()\n",
    "        # convert to lowercase\n",
    "        line = [word.lower() for word in line]\n",
    "        # remove punctuation from each token\n",
    "        #line = [word.translate(table) for word in line]\n",
    "        # remove non-printable chars form each token\n",
    "        #line = [re_print.sub('', w) for w in line]\n",
    "        # remove tokens with numbers in them\n",
    "        line = [word for word in line if word.isalpha()]\n",
    "        # store as string\n",
    "        clean_txt += (' '.join(line))\n",
    "    return clean_txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_txt = clean(train_ori_src)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"There is manuscript evidence that Austen continued to work on these pieces as late as the period 1809 â '' 11 , and that her niece and nephew , Anna and James Edward Austen , made further additions as late as 1814 .\\n\",\n",
       " \"In a remarkable comparative analysis , Mandaean scholar Säve-Söderberg demonstrated that Mani 's Psalms of Thomas were closely related to Mandaean texts .\\n\"]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ori_src[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'there is manuscript evidence that austen continued to work on these pieces as late as the period a and that her niece and nephew anna and james edward austen made further additions as late asin a remarkable comparative analysis mandaean scholar demonstrated that mani psalms of thomas were closely related to mandaean textsbefore persephone was released to hermes who had been sent to retrieve her ha'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_txt[0:400]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['there', 'is', 'manuscript', 'evidence', 'that', 'austen', 'continued', 'to', 'work', 'on', 'these', 'pieces', 'as', 'late', 'as', 'the', 'period', 'a', 'and', 'that'] \n",
      "\n",
      "Total tokens :  5742368\n"
     ]
    }
   ],
   "source": [
    "token_list_src = nltk.word_tokenize(clean_txt)\n",
    "print(token_list_src[0:20],\"\\n\")\n",
    "print(\"Total tokens : \", len(token_list_src))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['there', 'is', 'manuscript', 'evidence', 'that', 'continued', 'to', 'work', 'on', 'these', 'pieces', 'as', 'late', 'as', 'the', 'period', 'a', 'and', 'that', 'her'] \n",
      "\n",
      "Total tokens :  4961048\n"
     ]
    }
   ],
   "source": [
    "# comparison with source tokens when entities replaced by placeholders\n",
    "token_list_with_ent_src = nltk.word_tokenize(clean(train_src))\n",
    "print(token_list_with_ent_src[0:20],\"\\n\")\n",
    "print(\"Total tokens : \", len(token_list_with_ent_src))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "332341"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#total vocabulary size in source\n",
    "len(set(token_list_src))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Looking at the frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_list_src_Counter = collections.Counter(token_list_src)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 414529),\n",
       " ('of', 238408),\n",
       " ('and', 180703),\n",
       " ('in', 180597),\n",
       " ('a', 158684),\n",
       " ('is', 116580),\n",
       " ('to', 102777),\n",
       " ('was', 61810),\n",
       " ('as', 55788),\n",
       " ('by', 45144),\n",
       " ('for', 44558),\n",
       " ('on', 41885),\n",
       " ('with', 36286),\n",
       " ('from', 30777),\n",
       " ('that', 29024),\n",
       " ('an', 28936),\n",
       " ('or', 28105),\n",
       " ('are', 25374),\n",
       " ('at', 23806),\n",
       " ('his', 21935)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#most common vocabulary\n",
    "token_list_src_Counter.most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#list out of Counter\n",
    "token_list_src_Counter_desc = []\n",
    "for key, value in token_list_src_Counter.items():\n",
    "    token_list_src_Counter_desc.append((value, key))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "332341\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(1, 'statesshade'),\n",
       " (1, 'homelandhe'),\n",
       " (1, 'andhabilis'),\n",
       " (1, 'praiseit'),\n",
       " (1, 'europeavars'),\n",
       " (1, 'judthe'),\n",
       " (1, 'pragmaticis'),\n",
       " (1, 'bisnaga'),\n",
       " (1, 'plateaunathan'),\n",
       " (1, 'grandchildrenafter'),\n",
       " (1, 'wickboro'),\n",
       " (1, 'wced'),\n",
       " (1, 'mainlandwhile'),\n",
       " (1, 'foolishly'),\n",
       " (1, 'himprinted'),\n",
       " (1, 'bookbarbirolli'),\n",
       " (1, 'archivedle'),\n",
       " (1, 'francetijs'),\n",
       " (1, 'januaryoperationally'),\n",
       " (1, 'cyclonethese')]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#print least common\n",
    "print(len(token_list_src_Counter_desc))\n",
    "token_list_src_Counter_desc[-20:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45329\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(5, 'wastewater'),\n",
       " (5, 'insteadthis'),\n",
       " (5, 'mowing'),\n",
       " (5, 'veritable'),\n",
       " (5, 'uncritical'),\n",
       " (5, 'interstitial'),\n",
       " (5, 'infestation'),\n",
       " (5, 'kummer'),\n",
       " (5, 'jue'),\n",
       " (5, 'nae'),\n",
       " (5, 'respublik'),\n",
       " (5, 'photographerthe'),\n",
       " (5, 'sistere'),\n",
       " (5, 'salpeter'),\n",
       " (5, 'hakluyt'),\n",
       " (5, 'spainshe'),\n",
       " (5, 'brosseau'),\n",
       " (5, 'celluloid'),\n",
       " (5, 'dragonair'),\n",
       " (5, 'bistable')]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_size = 100 # GloVe 100d\n",
    "threshold = 4\n",
    "token_list_src_over_threshold = [(count, word) for (count, word) in token_list_src_Counter_desc if count > threshold ]\n",
    "token_list_src_over_threshold = sorted(token_list_src_over_threshold, key=lambda x: x[0])\n",
    "vocabulary_size_source = len(token_list_src_over_threshold) + 1\n",
    "print(vocabulary_size_source)\n",
    "token_list_src_over_threshold[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Which words/tokens from corpus are missing in GloVe; obviously lots of misspelled/not properly separated words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5916"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "not_in_GloVe = []\n",
    "for _, word in token_list_src_over_threshold:\n",
    "    if word not in embedding_index:\n",
    "        not_in_GloVe.append(word)\n",
    "len(not_in_GloVe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using spell checker a la Norvig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "alphabet = string.ascii_lowercase\n",
    "\n",
    "def edits1(word):\n",
    "    splits = [(word[:i], word[i:]) for i in range(len(word) + 1)]\n",
    "    #deletes = [a + b[1:] for a, b in splits if b]\n",
    "    #transposes = [a + b[1] + b[0] + b[2:] for a, b in splits if len(b)>1]\n",
    "    #replaces = [a + c + b[1:] for a, b in splits for c in alphabet if b]\n",
    "    #inserts = [a + c + b     for a, b in splits for c in alphabet]\n",
    "    #return set(splits + deletes + transposes + replaces + inserts)\n",
    "    return splits\n",
    "\n",
    "def separate_words(word):\n",
    "    cand = edits1(word)\n",
    "    for word1, word2 in cand:\n",
    "        if word1 in embedding_index and word2 in embedding_index:\n",
    "            return [word1, word2]\n",
    "        else:\n",
    "             continue\n",
    "    return [word]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Separating a word not in the embedding might give two words in the embedding. Run a couple of times, to see that the number of words not in GloVe decreases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3054"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for word in not_in_GloVe:\n",
    "    if len(separate_words(word))>1:\n",
    "        not_in_GloVe.remove(word)\n",
    "        \n",
    "len(not_in_GloVe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['respublik',\n",
       " 'sistere',\n",
       " 'deerthe',\n",
       " 'pruzhanov',\n",
       " 'returnthe',\n",
       " 'dunsshire',\n",
       " 'racingin',\n",
       " 'ageis',\n",
       " 'heroit',\n",
       " 'groherzogtum',\n",
       " 'atafter',\n",
       " 'opfertshofen',\n",
       " 'bryaxis',\n",
       " 'toxotidae',\n",
       " 'peoplean',\n",
       " 'lessines',\n",
       " 'presidenthe',\n",
       " 'septemberafter',\n",
       " 'membersin',\n",
       " 'connecticuta']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "not_in_GloVe[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def improve_token_list(token_list):\n",
    "    for word in token_list:\n",
    "        if word not in embedding_index and separate_words(word) != [word]:\n",
    "            token_list.remove(word)\n",
    "            token_list += separate_words(word)\n",
    "    return token_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "improved_token_list = improve_token_list([x[1] for x in token_list_src_over_threshold])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42118"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "improved_token_list = list((set(improved_token_list)))\n",
    "len(improved_token_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating token-index-dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_src = {}\n",
    "i = 1\n",
    "for word in improved_token_list:\n",
    "    tokenizer_src[word] = i\n",
    "    i += 1\n",
    "tokenizer_src[\"<UNK>\"] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42119"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer_src)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### embedding matrix to use the GloVe embedding in the model later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary_size_source = len(tokenizer_src)+1\n",
    "embedding_matrix = np.zeros((vocabulary_size_source, embedding_size))\n",
    "for word, idx in tokenizer_src.items():\n",
    "    embedding_vector = embedding_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[idx] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "from tensorflow.keras import preprocessing\n",
    "\n",
    "def tokenizer(line):\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    line = normalize('NFD', line).encode('ascii', 'ignore')\n",
    "    line = line.decode('UTF-8')\n",
    "    tokenized_line = tokenizer.tokenize(line)\n",
    "    tokenized_line = [word.lower() for word in tokenized_line]\n",
    "    return tokenized_line\n",
    "\n",
    "\n",
    "def encoder(tokenized_line, token_dict, seq_len):\n",
    "    encoded_line = []\n",
    "    for word in tokenized_line:\n",
    "        encoded_line.append(token_dict.get(word, token_dict[\"<UNK>\"]))\n",
    "    encoded_line = preprocessing.sequence.pad_sequences(\n",
    "    [encoded_line],\n",
    "    maxlen=seq_len,\n",
    "    padding=\"post\",\n",
    "    truncating=\"post\")\n",
    "\n",
    "    return encoded_line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Sirius left Portsmouth on 13 May 1787 , and arrived at Port Jackson on 26 January 1788 .\\n'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = train_ori_src[120]\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer(test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's look at sentence lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "lengths=[]\n",
    "lengths = [len((tokenizer(x))) for x in train_ori_src]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min 0\n",
      "mean 22.678625650299256\n",
      "max 104\n",
      "std 11.685398987054729\n"
     ]
    }
   ],
   "source": [
    "print(\"min\", np.min(lengths))\n",
    "print(\"mean\", np.mean(lengths))\n",
    "print(\"max\", np.max(lengths))\n",
    "print(\"std\", np.std(lengths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYMAAAD4CAYAAAAO9oqkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAV2klEQVR4nO3df6zd9X3f8edrdiEhbWIDHqO2md3hpjJoWegtuMpWpdCBgSjmDxqBuuFlVi2tJE27SIlppaElQYItKg1awuSBi4kifozSYgUS5hE2NKkYLiHlN+UGSHwtiG9iQ7pFgzh974/zMTu5uRf7nnN8f5z7fEhX93zf38/3nM9XX8uv+/l8P+ecVBWSpMXt7811ByRJc88wkCQZBpIkw0CShGEgSQKWznUHenXyySfXmjVr5robkrSgPPbYY9+vqhWT6ws2DNasWcPo6Ohcd0OSFpQk35mq7jSRJMkwkCQZBpIkDANJEoaBJImjCIMkO5LsT/LUpPrHkzyX5Okk/6GrflWSsSTPJ7mgq76x1caSbOuqr02yp9XvSHLcoE5OknR0jmZkcAuwsbuQ5DeBTcD7quoM4POtvh64DDijHfOlJEuSLAG+CFwIrAcub20BrgOur6rTgYPAln5PSpI0M0cMg6p6CDgwqfxvgGur6o3WZn+rbwJur6o3quolYAw4u/2MVdWLVfUmcDuwKUmAc4G72vE7gUv6PCdJ0gz1es/gl4F/1qZ3/meSX2v1lcDernbjrTZd/STgtao6NKk+pSRbk4wmGZ2YmOix65KkyXp9B/JS4ERgA/BrwJ1JfmlgvZpGVW0HtgOMjIz4rTyzZM22e996/PK1F89hTyQdK72GwThwd3W+Ju2RJH8HnAzsA1Z3tVvVakxT/wGwLMnSNjrobi9JmiW9ThP9JfCbAEl+GTgO+D6wC7gsyfFJ1gLrgEeAR4F1beXQcXRuMu9qYfIgcGl73s3APb2ejCSpN0ccGSS5DfggcHKSceBqYAewoy03fRPY3P5jfzrJncAzwCHgyqr6SXuejwH3A0uAHVX1dHuJTwO3J/kc8Dhw8wDPT5J0FI4YBlV1+TS7/sU07a8Brpmifh9w3xT1F+msNpIkzRHfgSxJMgwkSYaBJAnDQJKEYSBJwjCQJGEYSJIwDCRJGAaSJAwDSRKGgSQJw0CShGEgScIwkCRhGEiSMAwkSRgGkiSOIgyS7Eiyv33F5eR9n0xSSU5u20lyQ5KxJE8kOaur7eYkL7SfzV31X03yZDvmhiQZ1MlJko7O0YwMbgE2Ti4mWQ2cD3y3q3whsK79bAVubG1PpPPdyefQ+YrLq5Msb8fcCPxu13E/81qSpGPriGFQVQ8BB6bYdT3wKaC6apuAW6vjYWBZklOBC4DdVXWgqg4Cu4GNbd+7q+rhqirgVuCS/k5JkjRTPd0zSLIJ2FdVfz1p10pgb9f2eKu9XX18ivp0r7s1yWiS0YmJiV66LkmawozDIMkJwB8B/27w3Xl7VbW9qkaqamTFihWz/fKSNLR6GRn8I2At8NdJXgZWAd9M8g+AfcDqrrarWu3t6qumqEuSZtGMw6Cqnqyqv19Va6pqDZ2pnbOq6lVgF3BFW1W0AXi9ql4B7gfOT7K83Tg+H7i/7fthkg1tFdEVwD0DOjdJ0lE6mqWltwF/Bbw3yXiSLW/T/D7gRWAM+C/A7wFU1QHgs8Cj7eczrUZrc1M75tvA13o7FUlSr5YeqUFVXX6E/Wu6Hhdw5TTtdgA7pqiPAmceqR+SpGPHdyBLkgwDSZJhIEnCMJAkYRhIkjAMJEkYBpIkDANJEoaBJAnDQJKEYSBJwjCQJGEYSJIwDCRJGAaSJAwDSRKGgSSJo/vayx1J9id5qqv2H5M8l+SJJH+RZFnXvquSjCV5PskFXfWNrTaWZFtXfW2SPa1+R5LjBnmCw2TNtnvf+pGkQTqakcEtwMZJtd3AmVX1j4G/Aa4CSLIeuAw4ox3zpSRLkiwBvghcCKwHLm9tAa4Drq+q04GDwNt9x7KOwMCQ1IsjhkFVPQQcmFT7b1V1qG0+DKxqjzcBt1fVG1X1Ep0vuT+7/YxV1YtV9SZwO7ApSYBzgbva8TuBS/o8J0nSDA3insG/Br7WHq8E9nbtG2+16eonAa91Bcvh+pSSbE0ymmR0YmJiAF2XJEGfYZDkj4FDwFcG0523V1Xbq2qkqkZWrFgxGy8pSYvC0l4PTPKvgA8B51VVtfI+YHVXs1WtxjT1HwDLkixto4Pu9pKkWdLTyCDJRuBTwIer6kddu3YBlyU5PslaYB3wCPAosK6tHDqOzk3mXS1EHgQubcdvBu7p7VQkSb06mqWltwF/Bbw3yXiSLcB/An4B2J3kW0n+M0BVPQ3cCTwDfB24sqp+0v7q/xhwP/AscGdrC/Bp4N8mGaNzD+HmgZ6hJOmIjjhNVFWXT1Ge9j/sqroGuGaK+n3AfVPUX6Sz2kgz0L109OVrL57DnkgaBj3fM9D84XsKJPXLj6OQJDkymO/8q1/SbHBkIEkyDCRJThPNO04LSZoLhsE8YABImmtOE0mSHBksFr5JTdLbMQyGmNNPko6W00SSJMNAkmQYSJIwDCRJGAaSJAwDSRKGgSSJo/vayx1J9id5qqt2YpLdSV5ov5e3epLckGQsyRNJzuo6ZnNr/0KSzV31X03yZDvmhiQZ9Enqp63Zdu9bP5IERzcyuAXYOKm2DXigqtYBD7RtgAuBde1nK3AjdMIDuBo4h85XXF59OEBam9/tOm7ya0mSjrEjhkFVPQQcmFTeBOxsj3cCl3TVb62Oh4FlSU4FLgB2V9WBqjoI7AY2tn3vrqqHq6qAW7ueS5I0S3q9Z3BKVb3SHr8KnNIerwT2drUbb7W3q49PUZ9Skq1JRpOMTkxM9Nh1SdJkfd9Abn/R1wD6cjSvtb2qRqpqZMWKFbPxkpK0KPQaBt9rUzy03/tbfR+wuqvdqlZ7u/qqKeqSpFnUaxjsAg6vCNoM3NNVv6KtKtoAvN6mk+4Hzk+yvN04Ph+4v+37YZINbRXRFV3PJUmaJUf8COsktwEfBE5OMk5nVdC1wJ1JtgDfAT7Smt8HXASMAT8CPgpQVQeSfBZ4tLX7TFUdvin9e3RWLL0T+Fr7GXou65Q0nxwxDKrq8ml2nTdF2wKunOZ5dgA7pqiPAmceqR+SpGPHL7dZ5PwGNEngx1FIknBkoC6OEqTFy5GBJMkwkCQZBpIkDANJEt5A1jR8U5y0uDgykCQ5MphN/rUtab5yZCBJMgwkSYaBJAnDQJKEYSBJwtVEx5wriCQtBI4MJEn9hUGSP0zydJKnktyW5B1J1ibZk2QsyR1Jjmttj2/bY23/mq7nuarVn09yQX+nJEmaqZ7DIMlK4PeBkao6E1gCXAZcB1xfVacDB4Et7ZAtwMFWv761I8n6dtwZwEbgS0mW9NovSdLM9TtNtBR4Z5KlwAnAK8C5wF1t/07gkvZ4U9um7T8vSVr99qp6o6peAsaAs/vslyRpBnoOg6raB3we+C6dEHgdeAx4raoOtWbjwMr2eCWwtx17qLU/qbs+xTE/JcnWJKNJRicmJnrtuiRpkn6miZbT+at+LfCLwLvoTPMcM1W1vapGqmpkxYoVx/KlJGlR6Wea6LeAl6pqoqp+DNwNfABY1qaNAFYB+9rjfcBqgLb/PcAPuutTHCNJmgX9hMF3gQ1JTmhz/+cBzwAPApe2NpuBe9rjXW2btv8bVVWtfllbbbQWWAc80ke/JEkz1PObzqpqT5K7gG8Ch4DHge3AvcDtST7Xaje3Q24GvpxkDDhAZwURVfV0kjvpBMkh4Mqq+kmv/ZIkzVxf70CuqquBqyeVX2SK1UBV9X+B357mea4BrumnL5Kk3vkOZEmSYSBJMgwkSRgGkiT8CGvNUPdHcr987cVz2BNJg+TIQJJkGEiSDANJEt4zUB+8fyAND0cGkiTDQJJkGEiSMAwkSRgGkiQMA0kSLi3VgLjMVFrYHBlIkvoLgyTLktyV5Lkkzyb59SQnJtmd5IX2e3lrmyQ3JBlL8kSSs7qeZ3Nr/0KSzdO/oiTpWOh3ZPAF4OtV9SvA+4BngW3AA1W1DnigbQNcSOfL7tcBW4EbAZKcSOerM8+h83WZVx8OEEnS7Og5DJK8B/gN2hfeV9WbVfUasAnY2ZrtBC5pjzcBt1bHw8CyJKcCFwC7q+pAVR0EdgMbe+2XJGnm+hkZrAUmgD9L8niSm5K8Czilql5pbV4FTmmPVwJ7u44fb7Xp6j8jydYko0lGJyYm+ui6JKlbP6uJlgJnAR+vqj1JvsD/nxICoKoqSfXTwUnPtx3YDjAyMjKw5x207pU1krQQ9BMG48B4Ve1p23fRCYPvJTm1ql5p00D72/59wOqu41e12j7gg5Pq/6OPfmmOucxUWnh6niaqqleBvUne20rnAc8Au4DDK4I2A/e0x7uAK9qqog3A62066X7g/CTL243j81tNkjRL+n3T2ceBryQ5DngR+CidgLkzyRbgO8BHWtv7gIuAMeBHrS1VdSDJZ4FHW7vPVNWBPvslSZqBvsKgqr4FjEyx67wp2hZw5TTPswPY0U9fJEm98x3IkiTDQJLkB9XpGHNlkbQwODKQJBkGkiTDQJKEYSBJwjCQJOFqooHxw+mOzJVF0vzlyECSZBhIkgwDSRKGgSQJbyBrjngzWZpfHBlIkgwDSZJhIEliAGGQZEmSx5N8tW2vTbInyViSO9pXYpLk+LY91vav6XqOq1r9+SQX9NsnLSxrtt371o+kuTGIkcEngGe7tq8Drq+q04GDwJZW3wIcbPXrWzuSrAcuA84ANgJfSrJkAP2SJB2lvsIgySrgYuCmth3gXOCu1mQncEl7vKlt0/af19pvAm6vqjeq6iVgDDi7n35Jkmam36Wlfwp8CviFtn0S8FpVHWrb48DK9nglsBegqg4leb21Xwk83PWc3cf8lCRbga0Ap512Wp9d13zkklNpbvQ8MkjyIWB/VT02wP68raraXlUjVTWyYsWK2XpZSRp6/YwMPgB8OMlFwDuAdwNfAJYlWdpGB6uAfa39PmA1MJ5kKfAe4Add9cO6j5nXvOEpaVj0PDKoqquqalVVraFzA/gbVfU7wIPApa3ZZuCe9nhX26bt/0ZVVatf1lYbrQXWAY/02i9J0swdi4+j+DRwe5LPAY8DN7f6zcCXk4wBB+gECFX1dJI7gWeAQ8CVVfWTY9AvSdI00vnjfOEZGRmp0dHROe2D00Szx5vJ0mAkeayqRibXfQeyJMkwkCQZBpIkDANJEoaBJAm/6WzGXEEkaRgZBloQJoewS02lwXKaSJLkyEALk59uKg2WYaAFz2CQ+uc0kSTJMJAkGQaSJAwDSRLeQNaQ8Way1BtHBpIkRwZHw4+gkDTseh4ZJFmd5MEkzyR5OsknWv3EJLuTvNB+L2/1JLkhyViSJ5Kc1fVcm1v7F5Jsnu41JUnHRj/TRIeAT1bVemADcGWS9cA24IGqWgc80LYBLqTzZffrgK3AjdAJD+Bq4BzgbODqwwEiSZodPYdBVb1SVd9sj/8WeBZYCWwCdrZmO4FL2uNNwK3V8TCwLMmpwAXA7qo6UFUHgd3Axl77JUmauYHcM0iyBng/sAc4papeabteBU5pj1cCe7sOG2+16epTvc5WOqMKTjvttEF0fVreJ1j4pruGrjKSflbfYZDk54E/B/6gqn6Y5K19VVVJqt/X6Hq+7cB2gJGRkYE8r0sRJanPpaVJfo5OEHylqu5u5e+16R/a7/2tvg9Y3XX4qlabri5JmiX9rCYKcDPwbFX9SdeuXcDhFUGbgXu66le0VUUbgNfbdNL9wPlJlrcbx+e3mnRMrNl271s/kjr6mSb6APAvgSeTfKvV/gi4FrgzyRbgO8BH2r77gIuAMeBHwEcBqupAks8Cj7Z2n6mqA330S5I0Qz2HQVX9LyDT7D5vivYFXDnNc+0AdvTaF6lXx+KekfehtBAtyncgTzc94LSBpMXKzyaSJC3OkYE0Fad3tJgZBtIUDAYtNoaBdAQGgxYD7xlIkhwZSDPhKEHDyjCQemQwaJg4TSRJcmQgDYJvWNRC58hAkuTIQDqWvK+ghcKRgSTJkYE0W/waTs1nhoE0x5xK0nxgGEjziMGguWIYSPNUv8tVDRPNxLwJgyQbgS8AS4CbquraOe6StKB5j0IzMS/CIMkS4IvAPwfGgUeT7KqqZ+a2Z9Lw6WfEYZAMr3kRBsDZwFhVvQiQ5HZgE2AYSPOIQTK85ksYrAT2dm2PA+dMbpRkK7C1bf7vJM/3+HonA9/v8diFZLGcJ3iu816um/EhC/I8ezSb5/oPpyrOlzA4KlW1Hdje7/MkGa2qkQF0aV5bLOcJnuswWiznCfPjXOfLO5D3Aau7tle1miRpFsyXMHgUWJdkbZLjgMuAXXPcJ0laNObFNFFVHUryMeB+OktLd1TV08fwJfuealogFst5guc6jBbLecI8ONdU1Vz3QZI0x+bLNJEkaQ4ZBpKkxRUGSTYmeT7JWJJtc92fQUqyOsmDSZ5J8nSST7T6iUl2J3mh/V4+130dhCRLkjye5Ktte22SPe3a3tEWIix4SZYluSvJc0meTfLrw3hNk/xh+3f7VJLbkrxjWK5pkh1J9id5qqs25TVMxw3tnJ9IctZs9XPRhEHXR15cCKwHLk+yfm57NVCHgE9W1XpgA3BlO79twANVtQ54oG0Pg08Az3ZtXwdcX1WnAweBLXPSq8H7AvD1qvoV4H10znmormmSlcDvAyNVdSadRSSXMTzX9BZg46TadNfwQmBd+9kK3DhLfVw8YUDXR15U1ZvA4Y+8GApV9UpVfbM9/ls6/2mspHOOO1uzncAlc9PDwUmyCrgYuKltBzgXuKs1GZbzfA/wG8DNAFX1ZlW9xhBeUzorG9+ZZClwAvAKQ3JNq+oh4MCk8nTXcBNwa3U8DCxLcups9HMxhcFUH3mxco76ckwlWQO8H9gDnFJVr7RdrwKnzFG3BulPgU8Bf9e2TwJeq6pDbXtYru1aYAL4szYldlOSdzFk17Sq9gGfB75LJwReBx5jOK/pYdNdwzn7f2oxhcGikOTngT8H/qCqfti9rzrriBf0WuIkHwL2V9Vjc92XWbAUOAu4sareD/wfJk0JDck1XU7nL+K1wC8C7+Jnp1WG1ny5hospDIb+Iy+S/BydIPhKVd3dyt87PMxsv/fPVf8G5APAh5O8TGeq71w68+rL2hQDDM+1HQfGq2pP276LTjgM2zX9LeClqpqoqh8Dd9O5zsN4TQ+b7hrO2f9TiykMhvojL9q8+c3As1X1J127dgGb2+PNwD2z3bdBqqqrqmpVVa2hcw2/UVW/AzwIXNqaLfjzBKiqV4G9Sd7bSufR+Vj3obqmdKaHNiQ5of07PnyeQ3dNu0x3DXcBV7RVRRuA17umk46tqlo0P8BFwN8A3wb+eK77M+Bz+6d0hppPAN9qPxfRmU9/AHgB+O/AiXPd1wGe8weBr7bHvwQ8AowB/xU4fq77N6Bz/CfAaLuufwksH8ZrCvx74DngKeDLwPHDck2B2+jcC/kxndHelumuIRA6qx6/DTxJZ4XVrPTTj6OQJC2qaSJJ0jQMA0mSYSBJMgwkSRgGkiQMA0kShoEkCfh/GBbMWyWUc60AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.hist(lengths, bins=100)\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26105"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "607"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmin(lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'DreamWorks Animation has produced some of the highest grossing animated hits of all time , such as Antz -LRB-/O1998/O-RRB- , The Prince of Egypt -LRB-/O1998/O-RRB- , Shrek -LRB-/O2001/O-RRB- , its sequels Shrek 2 -LRB-/O2004/O-RRB- , Shrek the Third -LRB-/O2007/O-RRB- and Shrek Forever After -LRB-/O2010/O-RRB- ; Shark Tale -LRB-/O2004/O-RRB- , Madagascar -LRB-/O2005/O-RRB- , Over the Hedge -LRB-/O2006/O-RRB- , Flushed Away -LRB-/O2006/O-RRB- , Bee Movie -LRB-/O2007/O-RRB- , Kung Fu Panda -LRB-/O2008/O-RRB- and How to Train Your Dragon -LRB-/O2010/O-RRB- .\\n'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ori_src[26105]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"! ''\\n\""
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ori_src[607]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_length = 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = np.array([encoder(tokenizer(line), tokenizer_src, sentence_length) for line in train_ori_src]).reshape(-1, sentence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(296402, 60)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "lengths_dst = [len((tokenizer(x))) for x in train_ori_dst]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min 0\n",
      "mean 16.679013636885042\n",
      "max 80\n",
      "std 10.492046320896975\n"
     ]
    }
   ],
   "source": [
    "print(\"min\", np.min(lengths_dst))\n",
    "print(\"mean\", np.mean(lengths_dst))\n",
    "print(\"max\", np.max(lengths_dst))\n",
    "print(\"std\", np.std(lengths_dst))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.hist(lengths_dst, bins=100)\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_length_dst = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(296402, 60)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Look at output vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "301914"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_txt_dst = clean(train_ori_dst)\n",
    "token_list_dst = nltk.word_tokenize(clean_txt_dst)\n",
    "len(set(token_list_dst))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_list_dst_Counter = collections.Counter(token_list_dst)\n",
    "token_list_dst_Counter_desc = []\n",
    "for key, value in token_list_dst_Counter.items():\n",
    "    token_list_dst_Counter_desc.append((value, key))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34929\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(5, 'pomegranate'),\n",
       " (5, 'refineries'),\n",
       " (5, 'wastewater'),\n",
       " (5, 'scythe'),\n",
       " (5, 'italysome'),\n",
       " (5, 'pediapress'),\n",
       " (5, 'valentino'),\n",
       " (5, 'mantra'),\n",
       " (5, 'pst'),\n",
       " (5, 'predestination'),\n",
       " (5, 'waxy'),\n",
       " (5, 'solsticethe'),\n",
       " (5, 'barty'),\n",
       " (5, 'michener'),\n",
       " (5, 'statesevents'),\n",
       " (5, 'tokhtamysh'),\n",
       " (5, 'marilyns'),\n",
       " (5, 'connectivity'),\n",
       " (5, 'furius'),\n",
       " (5, 'camillus')]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_list_dst_Counter = collections.Counter(token_list_dst)\n",
    "embedding_size = 100 # GloVe 100d\n",
    "threshold = 4\n",
    "token_list_dst_over_threshold = [(count, word) for (count, word) in token_list_dst_Counter_desc if count > threshold ]\n",
    "token_list_dst_over_threshold = sorted(token_list_dst_over_threshold, key=lambda x: x[0])\n",
    "vocabulary_size_dst = len(token_list_dst_over_threshold) + 1\n",
    "print(vocabulary_size_dst)\n",
    "token_list_dst_over_threshold[:20]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32119\n"
     ]
    }
   ],
   "source": [
    "improved_token_list_dst = improve_token_list([x[1] for x in token_list_dst_over_threshold])\n",
    "improved_token_list_dst = list((set(improved_token_list_dst)))\n",
    "vocabulary_size_dst = len(improved_token_list_dst) + 1\n",
    "print(vocabulary_size_dst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32119"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_dst = {}\n",
    "i = 1\n",
    "for word in improved_token_list_dst:\n",
    "    tokenizer_dst[word] = i\n",
    "    i += 1\n",
    "tokenizer_dst[\"<UNK>\"] = i\n",
    "len(tokenizer_dst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y = np.array([encoder(tokenizer(line), tokenizer_dst, sentence_length_dst) for line in train_ori_dst]).reshape(-1, sentence_length_dst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(296402, 50)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18494"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 2)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "divmod(5,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_gen(data, b_size):\n",
    "    n = data.shape[0]\n",
    "    steps, rem = divmod(n, b_size)\n",
    "    for i in range(steps):\n",
    "        yield data[i*b_size:(i+1)*b_size]\n",
    "    yield data[(i+1)*b_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y_gen = train_gen(train_y, 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500, 50)"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(train_y_gen).shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_samples = \n",
    "decoder_targets_one_hot = np.zeros((\n",
    "        number_samples,\n",
    "        sentence_length_dst,\n",
    "        vocabulary_size_dst+1  # hm... why?\n",
    "    ))\n",
    "   \n",
    "\n",
    "def one_hot_encode_output(encoded_sentences, vocab_size):\n",
    "    for i, sentence in enumerate(encoded_sentences):\n",
    "        for t, word in enumerate(sentence):\n",
    "            decoder_targets_one_hot[i, t, word] = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y_gen = train_gen(train_y, 500)\n",
    "def one_hot_encode_output(encoded_sentences):      #, vocab_size):\n",
    "    for curr in encoded_sentences:         #encoded_sentences\n",
    "        number_samples = curr.shape[0]\n",
    "        decoder_targets_one_hot = np.zeros((\n",
    "                number_samples,\n",
    "                sentence_length_dst,\n",
    "                vocabulary_size_dst+1  # hm... why?\n",
    "            ))\n",
    "        for i, sentence in enumerate(curr):\n",
    "            for t, word in enumerate(sentence):\n",
    "                decoder_targets_one_hot[i, t, word] = 1\n",
    "        yield decoder_targets_one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode_output(encoded_sentences, b_size):\n",
    "    n = encoded_sentences.shape[0]\n",
    "    steps = n / b_size\n",
    "    i = 0\n",
    "    if i < steps:\n",
    "        curr = encoded_sentences[i*b_size:(i+1)*b_size]\n",
    "    else:\n",
    "        curr = encoded_sentences[(i+1)*b_size:]\n",
    "    \n",
    "    number_samples = curr.shape[0]\n",
    "    decoder_targets_one_hot = np.zeros((\n",
    "            number_samples,\n",
    "            sentence_length_dst,\n",
    "            vocabulary_size_dst+1  # hm... why?\n",
    "        ))\n",
    "    for i, sentence in enumerate(curr):\n",
    "        for t, word in enumerate(sentence):\n",
    "            decoder_targets_one_hot[i, t, word] = 1\n",
    "    i += 1\n",
    "    yield decoder_targets_one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "hm = one_hot_encode_output(train_y_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "593\n"
     ]
    }
   ],
   "source": [
    "# count = 0\n",
    "# while count < 1e6:\n",
    "#     next(hm).shape\n",
    "#     count += 1\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "296500"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "593*500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x_gen = train_gen(train_x, 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one hot encode target sequence\n",
    "def one_hot_encode_output(encoded_sentences, vocab_size):\n",
    "    ylist = list()\n",
    "    for i in range(encoded_sentences.shape[0]):\n",
    "        sequence = encoded_sentences[i]\n",
    "        encoded = to_categorical(sequence, num_classes=vocab_size)\n",
    "        ylist.append(encoded)\n",
    "    y = array(ylist)\n",
    "    y = y.reshape(encoded_sequences.shape[0], encoded_sequences.shape[1], vocab_size)\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'to_categorical' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-161-a167cb8616d4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mone_hot_encode_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_y\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocabulary_size_dst\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-159-98c9fcca7c4c>\u001b[0m in \u001b[0;36mone_hot_encode_output\u001b[0;34m(encoded_sentences, vocab_size)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoded_sentences\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0msequence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoded_sentences\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0mencoded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_categorical\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msequence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m         \u001b[0mylist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoded\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mylist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'to_categorical' is not defined"
     ]
    }
   ],
   "source": [
    "train_y = one_hot_encode_output(train_y[:1000], vocabulary_size_dst+1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def a():\n",
    "    for i in range(10):\n",
    "        yield i\n",
    "def b():\n",
    "    for i in range(10):\n",
    "        yield 2*i\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "a1 = a()\n",
    "b1 = b()\n",
    "def c():\n",
    "    for a2, b2 in zip(a1, b1):\n",
    "        yield (a2, b2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "c1 = zip(a1, b1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 4)"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(c1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_gen = zip(train_x_gen, one_hot_encode_output(train_y_gen))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator(X_data, y_data, batch_size):\n",
    "    samples_per_epoch = X_data.shape[0]\n",
    "    number_of_batches = samples_per_epoch/batch_size\n",
    "    counter=0\n",
    "\n",
    "    while 1:\n",
    "\n",
    "        X_batch = np.array(X_data[batch_size*counter:batch_size*(counter+1)]).astype('float32')\n",
    "        y_batch = np.array(y_data[batch_size*counter:batch_size*(counter+1)])#.astype('float32')\n",
    "        y_batch = one_hot_encode_output(y_batch)\n",
    "        counter += 1\n",
    "        yield X_batch,y_batch\n",
    "\n",
    "        #restart counter to yeild data in the next epoch as well\n",
    "        if counter >= number_of_batches:\n",
    "            counter = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "296402"
      ]
     },
     "execution_count": 267,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 268,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_targets_one_hot[1, 3, 943]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_targets_one_hot = np.zeros((\n",
    "            50,\n",
    "            sentence_length_dst,\n",
    "            vocabulary_size_dst+1  # hm... why?\n",
    "        ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode_output(encoded_sentences):      #, vocab_size):\n",
    "    curr = encoded_sentences        #encoded_sentences\n",
    "    number_samples = curr.shape[0]\n",
    "    decoder_targets_one_hot = np.zeros((\n",
    "            number_samples,\n",
    "            sentence_length_dst,\n",
    "            vocabulary_size_dst+1  # hm... why?\n",
    "        ))\n",
    "    for i, sentence in enumerate(curr):\n",
    "        for t, word in enumerate(sentence):\n",
    "            decoder_targets_one_hot[i, t, word] = 1\n",
    "    return decoder_targets_one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_19\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_19 (Embedding)     (None, 60, 100)           4212000   \n",
      "_________________________________________________________________\n",
      "lstm_38 (LSTM)               (None, 100)               80400     \n",
      "_________________________________________________________________\n",
      "repeat_vector_19 (RepeatVect (None, 50, 100)           0         \n",
      "_________________________________________________________________\n",
      "lstm_39 (LSTM)               (None, 50, 100)           80400     \n",
      "_________________________________________________________________\n",
      "time_distributed_19 (TimeDis (None, 50, 32120)         3244120   \n",
      "=================================================================\n",
      "Total params: 7,616,920\n",
      "Trainable params: 3,404,920\n",
      "Non-trainable params: 4,212,000\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "5929/5928 [==============================] - 15239s 3s/step - loss: 2.7005\n",
      "Epoch 2/10\n",
      "1587/5928 [=======>......................] - ETA: 2:56:38 - loss: 2.4797"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-276-e154df9d1168>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0;31m#batch_size=50,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;31m#validation_split=0.2,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m     \u001b[0msteps_per_epoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_x\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tristan/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1431\u001b[0m         \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1432\u001b[0m         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1433\u001b[0;31m         steps_name='steps_per_epoch')\n\u001b[0m\u001b[1;32m   1434\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1435\u001b[0m   def evaluate_generator(self,\n",
      "\u001b[0;32m~/anaconda3/envs/tristan/lib/python3.6/site-packages/tensorflow/python/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[0;34m(model, data, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch, mode, batch_size, steps_name, **kwargs)\u001b[0m\n\u001b[1;32m    262\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m       \u001b[0mis_deferred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_compiled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 264\u001b[0;31m       \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    265\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tristan/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight, reset_metrics)\u001b[0m\n\u001b[1;32m   1173\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_sample_weight_modes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_weights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1174\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1175\u001b[0;31m       \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1177\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreset_metrics\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tristan/lib/python3.6/site-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3290\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3291\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[0;32m-> 3292\u001b[0;31m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[1;32m   3293\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3294\u001b[0m     output_structure = nest.pack_sequence_as(\n",
      "\u001b[0;32m~/anaconda3/envs/tristan/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1456\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1457\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1458\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1459\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1460\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import models, layers, callbacks\n",
    "import datetime\n",
    "#import tensorboard\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(layers.Embedding(          #word embedding\n",
    "    vocabulary_size_source,\n",
    "    embedding_size,\n",
    "    input_length=sentence_length\n",
    "\n",
    "))\n",
    "model.add(layers.LSTM(embedding_size))\n",
    "model.add(layers.RepeatVector(sentence_length_dst))\n",
    "model.add(layers.LSTM(embedding_size, return_sequences=True))\n",
    "model.add(layers.TimeDistributed(layers.Dense(vocabulary_size_dst+1, activation='softmax')))\n",
    "\n",
    "\n",
    "model.layers[0].set_weights([embedding_matrix])\n",
    "model.layers[0].trainable = False   #freeze\n",
    "\n",
    "model.summary()\n",
    "\n",
    "\n",
    "\n",
    "model.compile(\n",
    "    optimizer=\"adam\",\n",
    "    loss=\"categorical_crossentropy\",\n",
    "    #metrics=[\"accuracy\"]\n",
    "\n",
    ")\n",
    "batch_size = 50\n",
    "history = model.fit_generator(\n",
    "    generator(train_x, train_y, batch_size),\n",
    "    #data_gen,\n",
    "    #(train_gen(train_x, 500), one_hot_encode_output(train_y_gen)),\n",
    "    epochs=10,\n",
    "    #batch_size=50,\n",
    "    #validation_split=0.2,\n",
    "    steps_per_epoch = train_x.shape[0]/batch_size,\n",
    "    \n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tristan",
   "language": "python",
   "name": "tristan"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
